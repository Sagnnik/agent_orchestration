{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cc73f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured output works: queries=[PlannedQuery(query='What is AI in simple terms?', tools=[<ResearchTool.WEBSCRAPER: 'webscraper'>, <ResearchTool.WIKIPEDIA: 'wikipedia'>, <ResearchTool.TAVILY: 'tavily'>]), PlannedQuery(query='Real-world examples of AI you use daily', tools=[<ResearchTool.WEBSCRAPER: 'webscraper'>, <ResearchTool.TAVILY: 'tavily'>]), PlannedQuery(query='Current AI vs. human intelligence (narrow vs. general)', tools=[<ResearchTool.WIKIPEDIA: 'wikipedia'>, <ResearchTool.TAVILY: 'tavily'>]), PlannedQuery(query='Why do people confuse AI with consciousness?', tools=[<ResearchTool.TAVILY: 'tavily'>])] rationals='This plan focuses on: 1) Simple definitions (avoids jargon), 2) Concrete examples (relatable to daily life), 3) Current reality (narrow AI dominates), 4) Common misconceptions (consciousness). All queries target beginner-friendly answers without technical depth. Tools like Tavily provide quick, accurate responses while web scrapers gather real-world context.'\n"
     ]
    }
   ],
   "source": [
    "from core.llm import get_llm\n",
    "from core.llm_response_models import QueryPlanOutput\n",
    "\n",
    "model = get_llm(provider=\"ollama\", model_name=\"qwen3:4b\")\n",
    "structured_model = model.with_structured_output(QueryPlanOutput)\n",
    "\n",
    "# Test with a simple prompt\n",
    "test_prompt = \"Generate a simple query plan for 'What is AI?'\"\n",
    "try:\n",
    "    response = structured_model.invoke([test_prompt])\n",
    "    print(\"Structured output works:\", response)\n",
    "except Exception as e:\n",
    "    print(\"Error with structured output:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010bee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: ollama/qwen3:4b\n",
      "graph created...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from core.agent import create_agent_streaming, create_agent\n",
    "from core.graph import create_graph\n",
    "import asyncio\n",
    "from uuid import uuid4\n",
    "from core.llm_response_models import ResearchDepth\n",
    "\n",
    "model_provider: str=\"ollama\"\n",
    "model_name:str='qwen3:4b'\n",
    "graph = create_graph(model_provider, model_name)\n",
    "app = graph.compile()\n",
    "from IPython.display import Image, display\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b035fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.post('http://localhost:11434/api/generate', \n",
    "    json={\n",
    "        'model': 'qwen3:4b',\n",
    "        'prompt': 'Say hello',\n",
    "        'stream': False\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789dd42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.graph import create_graph\n",
    "from uuid import uuid4\n",
    "from core.llm_response_models import ResearchDepth\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "load_dotenv()\n",
    "\n",
    "model_provider: str=\"openai\"\n",
    "model_name:str='gpt-4o-mini'\n",
    "\n",
    "\n",
    "app = create_graph(model_provider, model_name)\n",
    "\n",
    "thread_id = uuid4().hex\n",
    "query = \"Explain transformer architecture in AI\"\n",
    "research_depth = ResearchDepth.MODERATE\n",
    "\n",
    "initial_state = {\n",
    "    \"original_query\": query,\n",
    "    \"depth\": research_depth,\n",
    "    \"iteration_count\": 0,\n",
    "    \"max_iterations\": 5, \n",
    "    \"is_complete\": False,\n",
    "    \"search_results\": [],\n",
    "}\n",
    "\n",
    "# try:\n",
    "#     print(\"Invoking app\")\n",
    "#     print(f\"Initial state: {initial_state}\")\n",
    "#     result = app.invoke(initial_state, {\"configurable\": {\"thread_id\": thread_id}})\n",
    "#     print(\"=\" * 50)\n",
    "#     print(\"FINAL RESULT:\")\n",
    "#     print(result)\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {str(e)}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea4af401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResearchDepth.MODERATE\n"
     ]
    }
   ],
   "source": [
    "from core.llm_response_models import ResearchDepth\n",
    "from uuid import uuid4\n",
    "research_depth = ResearchDepth.MODERATE\n",
    "thread_id = uuid4().hex\n",
    "query = \"Explain transformer architecture in AI\"\n",
    "initial_state = {\n",
    "    \"original_query\": query,\n",
    "    \"depth\": research_depth,\n",
    "    \"iteration_count\": 0,\n",
    "    \"max_iterations\": 5, \n",
    "    \"is_complete\": False,\n",
    "    \"search_results\": [],\n",
    "}\n",
    "print(initial_state['depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71978b",
   "metadata": {},
   "source": [
    "## Testing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84da390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tool:  ResearchTool.WIKIPEDIA\n",
      "Using tool:  ResearchTool.TAVILY\n",
      "Using tool:  ResearchTool.TAVILY\n",
      "Using tool:  ResearchTool.ARXIV\n",
      "Using tool:  ResearchTool.ARXIV\n",
      "Using tool:  ResearchTool.TAVILY\n"
     ]
    }
   ],
   "source": [
    "from core.tools.wikipedia_search import wikipedia_search\n",
    "from core.tools.arxiv_search import arxiv_search\n",
    "from core.tools.tavily_search import tavily_search_tool\n",
    "from core.llm_response_models import PlannedQuery, ResearchTool, SearchQueryResult\n",
    "from core.graph import get_source_type, parse_tool_results\n",
    "queries=[\n",
    "    PlannedQuery(\n",
    "        query='Overview of transformer architecture in artificial intelligence', \n",
    "        tools=[ResearchTool.WIKIPEDIA, ResearchTool.TAVILY]\n",
    "    ), \n",
    "    PlannedQuery(\n",
    "        query='Recent advancements in transformer models for natural language processing', \n",
    "        tools=[ResearchTool.TAVILY, ResearchTool.ARXIV]\n",
    "    ),\n",
    "    PlannedQuery(\n",
    "        query='Comparative analysis of transformer architecture vs. recurrent neural networks', \n",
    "        tools=[ResearchTool.ARXIV, ResearchTool.TAVILY]\n",
    "    )\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "TOOL_FUNCTIONS = {\n",
    "    ResearchTool.TAVILY: tavily_search_tool,\n",
    "    ResearchTool.WIKIPEDIA: wikipedia_search,\n",
    "    ResearchTool.ARXIV: arxiv_search,\n",
    "}\n",
    "\n",
    "for q in queries:\n",
    "    for tool in q.tools:\n",
    "        try:\n",
    "            print(\"Using tool: \", tool)\n",
    "            tool_input = q.query\n",
    "            raw_result = TOOL_FUNCTIONS[tool](tool_input)\n",
    "\n",
    "            search_result = SearchQueryResult(\n",
    "                            query=q.query,\n",
    "                            tool=tool,\n",
    "                            source_type=get_source_type(tool),\n",
    "                            results=parse_tool_results(raw_result, tool)\n",
    "                        )\n",
    "            all_results.append(search_result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing tool: {str(e)}\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e108c2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SearchQueryResult(query='Overview of transformer architecture in artificial intelligence', tool=<ResearchTool.WIKIPEDIA: 'wikipedia'>, source_type=<SourceType.WIKIPEDIA: 'wikipedia'>, timestamp=datetime.datetime(2025, 11, 17, 18, 15, 1, 987916), results=[SearchResult(title='Generative artificial intelligence', url='https://en.wikipedia.org/wiki/Generative_artificial_intelligence', content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora.', raw_content=None, score=None, date=None, metadata={'pageid': '73291755', 'revision_id': 1322124483, 'categories': ['2020s fads and trends', '2020s in computing', '2023 in computing', '2024 in computing', '2025 in computing'], 'content_length': 39569}), SearchResult(title='Hallucination (artificial intelligence)', url='https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)', content='In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called bullshitting, confabulation, or delusion) is a response generated by AI that contains false or misleading information presented as fact. This term draws a loose analogy with human psychology, where a hallucination typically involves false percepts. However, there is a key difference: AI hallucination is associated with erroneously constructed responses (confabulation), rather than perceptual experiences. \\nFor example, a chatbot powered by large language models (LLMs), like ChatGPT, may embed plausible-sounding random falsehoods within its generated content. Detecting and mitigating errors and hallucinations pose significant challenges for practical deployment and reliability of LLMs in high-stakes scenarios, such as chip design, supply chain logistics, and medical diagnostics.', raw_content=None, score=None, date=None, metadata={'pageid': '72607666', 'revision_id': 1322508065, 'categories': ['All articles with unsourced statements', 'Anthropomorphism', 'Articles containing video clips', 'Articles with short description', 'Articles with unsourced statements from March 2024'], 'content_length': 31596}), SearchResult(title='Artificial intelligence', url='https://en.wikipedia.org/wiki/Artificial_intelligence', content='Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.', raw_content=None, score=None, date=None, metadata={'pageid': '1164', 'revision_id': 1322626785, 'categories': ['All accuracy disputes', 'Articles with Internet Encyclopedia of Philosophy links', 'Articles with disputed statements from July 2024', 'Articles with excerpts', 'Articles with short description'], 'content_length': 91104}), SearchResult(title='Transformer (deep learning)', url='https://en.wikipedia.org/wiki/Transformer_(deep_learning)', content='In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. \\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.\\n\\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google.', raw_content=None, score=None, date=None, metadata={'pageid': '61603971', 'revision_id': 1321915496, 'categories': ['2017 in artificial intelligence', 'Articles with short description', 'Google software', 'Neural network architectures', 'Short description is different from Wikidata'], 'content_length': 112856}), SearchResult(title='Glossary of artificial intelligence', url='https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence', content='This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence (AI), its subdisciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, Glossary of machine vision, and Glossary of logic.\\n\\n\\n== A ==\\n\\nA* search\\nPronounced \"A-star\".\\nA graph traversal and pathfinding algorithm which is used in many fields of computer science due to its completeness, optimality, and optimal efficiency.\\n\\nabductive logic programming (ALP)\\nA high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning.', raw_content=None, score=None, date=None, metadata={'pageid': '50336055', 'revision_id': 1319751840, 'categories': ['All articles with dead external links', 'All articles with unsourced statements', 'Articles with dead external links from July 2022', 'Articles with dead external links from September 2023', 'Articles with permanently dead external links'], 'content_length': 132687})]), SearchQueryResult(query='Overview of transformer architecture in artificial intelligence', tool=<ResearchTool.TAVILY: 'tavily'>, source_type=<SourceType.WEB: 'web'>, timestamp=datetime.datetime(2025, 11, 17, 18, 15, 3, 13744), results=[SearchResult(title='LLM Transformer Model Visually Explained', url='https://poloclub.github.io/transformer-explainer/', content='Transformer is a neural network architecture that has fundamentally changed the approach to Artificial Intelligence. Transformer was first introduced in the seminal paper \"Attention is All You Need\" in 2017 and has since become the go-to architecture for deep learning models, powering text-generative models like OpenAI\\'s GPT, Meta\\'s Llama, and Google\\'s Gemini. Beyond text, Transformer is also applied in audio generation, image recognition, protein structure prediction, and even game playing,', raw_content=None, score=0.86413884, date=None, metadata={}), SearchResult(title='Transformer (deep learning) - Wikipedia', url='https://en.wikipedia.org/wiki/Transformer_(deep_learning)', content='In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention \"Attention (machine learning)\") mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token \"Tokenization (lexical analysis)\") is then contextualized \"Contextualization (computer science)\") within the scope of the context window with other (unmasked) [...] The attention mechanism used in the transformer architecture are scaled dot-product attention \"Attention (machine learning)\") units. For each unit, the transformer model learns three weight matrices: the query weights      W  Q     {\\\\displaystyle W^{Q}}  {\\\\displaystyle W^{Q}}, the key weights      W  K     {\\\\displaystyle W^{K}}  {\\\\displaystyle W^{K}}, and the value weights      W  V     {\\\\displaystyle W^{V}}  {\\\\displaystyle W^{V}}. [...] As the transformer architecture natively consists of operations over numbers (matrix multiplications, dot products, activation functions) rather than over text, there must first be a mapping from any input text to some numerical representation. This happens in three steps.', raw_content=None, score=0.81025416, date=None, metadata={}), SearchResult(title='What is a Transformer Model? | IBM', url='https://www.ibm.com/think/topics/transformer-model', content='The transformer model is a type of neural network architecture that excels at processing sequential data, most prominently associated with large language models (LLMs). Transformer models have also achieved elite performance in other fields of artificial intelligence (AI), such as computer vision, speech recognition and time series forecasting. [...] The central feature of transformer models is their self-attention mechanism, from which transformer models derive their impressive ability to detect the relationships (or dependencies) between each part of an input sequence. Unlike the RNN and CNN architectures that preceded it, the transformer architecture uses only attention layers and standard feedforward layers. [...] The transformer architecture was first described in the seminal 2017 paper “Attention is All You Need” by Vaswani and others, which is now considered a watershed moment in deep learning.  \\n   \\n Originally introduced as an evolution of the recurrent neural network (RNN)-based sequence-to-sequence models used for machine translation, transformer-based models have since attained cutting-edge advancements across nearly every machine learning (ML) discipline.', raw_content=None, score=0.8047902, date=None, metadata={}), SearchResult(title='What are Transformers in Artificial Intelligence? - Amazon AWS', url='https://aws.amazon.com/what-is/transformers-in-artificial-intelligence/', content='Transformers are a type of neural network architecture that transforms or changes an input sequence into an output sequence. They do this by learning context and tracking relationships between sequence components. For example, consider this input sequence: \"What is the color of the sky?\" The transformer model uses an internal mathematical representation that identifies the relevancy and relationship between the words color, sky, and blue. It uses that knowledge to generate the output: \"The sky [...] ## What are the components of transformer architecture?\\n\\nTransformer neural network architecture has several software layers that work together to generate the final output. The following image shows the components of transformation architecture, as explained in the rest of this section.\\n\\n### Input embeddings [...] Vision transformers (ViT) repurpose the transformer architecture for image classification tasks. Instead of processing an image as a grid of pixels, they view image data as a sequence of fixed-size patches, similar to how words are treated in a sentence. Each patch is flattened, linearly embedded, and then processed sequentially by the standard transformer encoder. Positional embeddings are added to maintain spatial information. This usage of global self-attention enables the model to capture', raw_content=None, score=0.78572035, date=None, metadata={}), SearchResult(title='How Transformers Work: A Detailed Exploration of ... - DataCamp', url='https://www.datacamp.com/tutorial/how-transformers-work', content=\"First appeared in 2017 in the “Attention is all you need” article by Google, the transformer architecture is at the heart of groundbreaking models like ChatGPT, sparking a new wave of excitement in the AI community. They've been instrumental in OpenAI's cutting-edge language models and played a key role in DeepMind's AlphaStar.\\n\\nIn this transformative era of AI, the significance of Transformer models for aspiring data scientists and NLP practitioners cannot be overstated. [...] A transformer is a type of artificial intelligence model that learns to understand and generate human-like text by analyzing patterns in large amounts of text data.\\n\\nTransformers are a current state-of-the-art NLP model and are considered the evolution of the encoder-decoder architecture. However, while the encoder-decoder architecture relies mainly on Recurrent Neural Networks (RNNs) to extract sequential information, Transformers completely lack this recurrency.\\n\\nSo, how do they do it? [...] The shift from Recurrent Neural Networks (RNNs) like LSTM to Transformers in NLP is driven by these two main problems and Transformers' ability to assess both of them by taking advantage of the Attention mechanism improvements:\\n\\n Pay attention to specific words, no matter how distant they are.\\n Boost the performance speed.\\n\\nThus, Transformers became a natural improvement of RNNs.\\n\\nNext, let’s take a look at how transformers work.\\n\\n## The Transformer Architecture\\n\\n### Overview\", raw_content=None, score=0.78154784, date=None, metadata={})]), SearchQueryResult(query='Recent advancements in transformer models for natural language processing', tool=<ResearchTool.TAVILY: 'tavily'>, source_type=<SourceType.WEB: 'web'>, timestamp=datetime.datetime(2025, 11, 17, 18, 15, 7, 646674), results=[SearchResult(title='Transformer Models in Natural Language Processing - Netguru', url='https://www.netguru.com/blog/transformer-models-in-nlp', content='Research collaborations between academia and industry have yielded specialized transformers for speech recognition that achieve word error rates below 5% in many languages. These developments often emerge from collaborative efforts spanning multiple institutions. [...] Natural Language Processing (NLP) represents the original and most mature application of transformer models. When introduced in 2017, transformers quickly surpassed previous approaches to machine translation, establishing new benchmarks in accuracy and fluency.\\n\\nThe self-attention mechanism allows transformers to grasp context and relationships between words in a sentence, regardless of their distance from each other. This capability proves crucial for tasks like: [...] The key innovation was self-attention, allowing the model to weigh the importance of words against each other regardless of their position in the text. This enabled truly parallel processing, making training much faster and more efficient.\\n\\nThe original Transformer quickly led to powerful models like BERT (2018) and GPT (2018), which demonstrated unprecedented abilities in understanding and generating human language. These models scaled effectively with more data and larger parameter counts.', raw_content=None, score=0.9997527, date=None, metadata={}), SearchResult(title='[PDF] Advanced Transformer-Based Deep Learning Techniques For ...', url='https://africanjournalofbiomedicalresearch.com/index.php/AJBR/article/download/4562/3507/8591', content='Enhancing Contextual Understanding In Natural Language Processing 5155 Afr. J. Biomed. Res. Vol. 27, No.4s (December) 2024 Dr M Nithin Varma et al Additionally, reversible layers and memory-efficient encoding strategies have been proposed to reduce the computational burden during training and inference. These developments highlight the ongoing efforts to make transformer models more scalable and efficient while preserving their contextual capabilities. The integration of external knowledge has [...] Accuracy 78.00% 85.00% 9.00% Advanced Transformer-Based Deep Learning Techniques For Enhancing Contextual Understanding In Natural Language Processing 5160 Afr. J. Biomed. Res. Vol. 27, No.4s (December) 2024 Dr M Nithin Varma et al Figure 6: Performance Trends Across Tasks The findings confirm that the proposed transformer-based model addresses key limitations of existing architectures, demonstrating superior contextual understanding, computational efficiency, and adaptability. These results [...] and dynamic encoding strategies(Murali Krishnam Raju et al., 2022). This study aims to bridge these gaps by proposing a novel transformer-based architecture that incorporates these advancements to enhance performance and contextual understanding across diverse NLP tasks. 3. Methodology This section outlines the systematic approach adopted for designing, training, and evaluating the proposed advanced transformer-based model. Key components include architectural enhancements, data preparation,', raw_content=None, score=0.99940693, date=None, metadata={}), SearchResult(title='The Transformer Model: Revolutionizing Natural Language ...', url='https://medium.com/@lordmoma/the-transformer-model-revolutionizing-natural-language-processing-a16be54ddb1e', content='The Transformer model is a powerful deep learning architecture that has revolutionized the field of natural language processing. Its ability to handle long-range dependencies in text data makes it well-suited for a wide range of NLP tasks. The tokenizer, embedding, positional encoding, Transformer block, attention, feedforward, and softmax components work together to enable the model to guess the next word in a text sequence with remarkable accuracy. With continued advances in deep learning [...] research, the Transformer model is poised to remain at the forefront of NLP for years to come. [...] Sign up\\n\\nSign in\\n\\nSign up\\n\\nSign in\\n\\n# The Transformer Model: Revolutionizing Natural Language Processing\\n\\nDavid Lee\\n\\n--\\n\\nListen\\n\\nShare\\n\\nThe Transformer model is a deep learning architecture that has revolutionized the field of natural language processing (NLP). It is based on a self-attention mechanism that allows it to handle long-range dependencies in text data, making it well-suited for tasks such as language translation, summarization, and question-answering.', raw_content=None, score=0.9989434, date=None, metadata={}), SearchResult(title='What is a Transformer Model? | IBM', url='https://www.ibm.com/think/topics/transformer-model', content='Transformer models are most commonly associated with NLP, having originally been developed for machine translation use cases. Most notably, the transformer architecture gave rise to the large language models (LLMs) that catalyzed the advent of generative AI.\\n\\nMost of the LLMs that the public is most familiar with, from closed source models such as OpenAI’s GPT series and Anthropic’s Claude models to open source models including Meta Llama or IBM® Granite®, are autoregressive decoder-only LLMs. [...] The transformer model is a type of neural network architecture that excels at processing sequential data, most prominently associated with large language models (LLMs). Transformer models have also achieved elite performance in other fields of artificial intelligence (AI), such as computer vision, speech recognition and time series forecasting. [...] ## Transformer models in other fields\\n\\nThough transformer models were originally designed for, and continue to be most prominently associated with natural language use cases, they can be used in nearly any situation involving sequential data. This has led to the development of transformer-based models in other fields, from fine-tuning LLMs into multimodal systems to dedicated time series forecasting models and ViTs for computer vision.', raw_content=None, score=0.99694854, date=None, metadata={}), SearchResult(title='Transformers: State-of-the-Art Natural Language Processing', url='https://aclanthology.org/2020.emnlp-demos.6/', content='Transformers: State-of-the-Art Natural Language Processing (Wolf et al., EMNLP 2020)\\n\\n##### ACL [...] ACL Logo\\n\\n## Transformers: State-of-the-Art Natural Language Processing\\n\\nThomas Wolf,\\nLysandre Debut,\\nVictor Sanh,\\nJulien Chaumond,\\nClement Delangue,\\nAnthony Moi,\\nPierric Cistac,\\nTim Rault,\\nRemi Louf,\\nMorgan Funtowicz,\\nJoe Davison,\\nSam Shleifer,\\nPatrick von Platen,\\nClara Ma,\\nYacine Jernite,\\nJulien Plu,\\nCanwen Xu,\\nTeven Le Scao,\\nSylvain Gugger,\\nMariama Drame,\\nQuentin Lhoest,\\nAlexander Rush\\n\\n##### Correct Metadata for\\n\\n##### Abstract\\n\\n##### Export citation\\n\\n##### Markdown (Informal) [...] The ACL Anthology is managed and built by the ACL Anthology team of volunteers.\\n\\nSite last built on 14 November 2025 at 22:36 UTC with commit b62994a.', raw_content=None, score=0.99108386, date=None, metadata={})]), SearchQueryResult(query='Recent advancements in transformer models for natural language processing', tool=<ResearchTool.ARXIV: 'arxiv'>, source_type=<SourceType.ARXIV: 'arxiv'>, timestamp=datetime.datetime(2025, 11, 17, 18, 15, 7, 841672), results=[SearchResult(title='An Open Natural Language Processing Development Framework for EHR-based Clinical Research: A case demonstration using the National COVID Cohort Collaborative (N3C)', url='http://arxiv.org/abs/2110.10780v3', content=\"While we pay attention to the latest advances in clinical natural language processing (NLP), we can notice some resistance in the clinical and translational research community to adopt NLP models due to limited transparency, interpretability, and usability. In this study, we proposed an open natural language processing development framework. We evaluated it through the implementation of NLP algorithms for the National COVID Cohort Collaborative (N3C). Based on the interests in information extraction from COVID-19 related clinical notes, our work includes 1) an open data annotation process using COVID-19 signs and symptoms as the use case, 2) a community-driven ruleset composing platform, and 3) a synthetic text data generation workflow to generate texts for information extraction tasks without involving human subjects. The corpora were derived from texts from three different institutions (Mayo Clinic, University of Kentucky, University of Minnesota). The gold standard annotations were tested with a single institution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706, and 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets, respectively. The study as a consortium effort of the N3C NLP subgroup demonstrates the feasibility of creating a federated NLP algorithm development and benchmarking platform to enhance multi-institution clinical NLP study and adoption. Although we use COVID-19 as a use case in this effort, our framework is general enough to be applied to other domains of interest in clinical NLP.\", raw_content=None, score=None, date=None, metadata={'authors': ['Sijia Liu', 'Andrew Wen', 'Liwei Wang', 'Huan He', 'Sunyang Fu', 'Robert Miller', 'Andrew Williams', 'Daniel Harris', 'Ramakanth Kavuluru', 'Mei Liu', 'Noor Abu-el-rub', 'Dalton Schutte', 'Rui Zhang', 'Masoud Rouhizadeh', 'John D. Osborne', 'Yongqun He', 'Umit Topaloglu', 'Stephanie S Hong', 'Joel H Saltz', 'Thomas Schaffter', 'Emily Pfaff', 'Christopher G. Chute', 'Tim Duong', 'Melissa A. Haendel', 'Rafael Fuentes', 'Peter Szolovits', 'Hua Xu', 'Hongfang Liu', 'National COVID Cohort Collaborative', 'Natural Language Processing', 'Subgroup', 'National COVID Cohort Collaborative'], 'published': '2021-10-20', 'categories': ['cs.CL', 'cs.IR'], 'primary_category': 'cs.CL', 'pdf_url': 'https://arxiv.org/pdf/2110.10780v3'}), SearchResult(title='A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models', url='http://arxiv.org/abs/2107.03844v3', content='Bangla -- ranked as the 6th most widely spoken language across the world (https://www.ethnologue.com/guides/ethnologue200), with 230 million native speakers -- is still considered as a low-resource language in the natural language processing (NLP) community. With three decades of research, Bangla NLP (BNLP) is still lagging behind mainly due to the scarcity of resources and the challenges that come with it. There is sparse work in different areas of BNLP; however, a thorough survey reporting previous work and recent advances is yet to be done. In this study, we first provide a review of Bangla NLP tasks, resources, and tools available to the research community; we benchmark datasets collected from various platforms for nine NLP tasks using current state-of-the-art algorithms (i.e., transformer-based models). We provide comparative results for the studied NLP tasks by comparing monolingual vs. multilingual models of varying sizes. We report our results using both individual and consolidated datasets and provide data splits for future research. We reviewed a total of 108 papers and conducted 175 sets of experiments. Our results show promising performance using transformer-based models while highlighting the trade-off with computational costs. We hope that such a comprehensive survey will motivate the community to build on and further advance the research on Bangla NLP.', raw_content=None, score=None, date=None, metadata={'authors': ['Firoj Alam', 'Arid Hasan', 'Tanvirul Alam', 'Akib Khan', 'Janntatul Tajrin', 'Naira Khan', 'Shammur Absar Chowdhury'], 'published': '2021-07-08', 'categories': ['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG'], 'primary_category': 'cs.CL', 'pdf_url': 'https://arxiv.org/pdf/2107.03844v3'}), SearchResult(title='A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text', url='http://arxiv.org/abs/2306.06371v1', content=\"Java Code Generation consists in generating automatically Java code from a Natural Language Text. This NLP task helps in increasing programmers' productivity by providing them with immediate solutions to the simplest and most repetitive tasks. Code generation is a challenging task because of the hard syntactic rules and the necessity of a deep understanding of the semantic aspect of the programming language. Many works tried to tackle this task using either RNN-based, or Transformer-based models. The latter achieved remarkable advancement in the domain and they can be divided into three groups: (1) encoder-only models, (2) decoder-only models, and (3) encoder-decoder models. In this paper, we provide a comprehensive review of the evolution and progress of deep learning models in Java code generation task. We focus on the most important methods and present their merits and limitations, as well as the objective functions used by the community. In addition, we provide a detailed description of datasets and evaluation metrics used in the literature. Finally, we discuss results of different models on CONCODE dataset, then propose some future directions.\", raw_content=None, score=None, date=None, metadata={'authors': ['Jessica López Espejel', 'Mahaman Sanoussi Yahaya Alassan', 'El Mehdi Chouham', 'Walid Dahhane', 'El Hassane Ettifouri'], 'published': '2023-06-10', 'categories': ['cs.CL'], 'primary_category': 'cs.CL', 'pdf_url': 'https://arxiv.org/pdf/2306.06371v1'}), SearchResult(title='SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks', url='http://arxiv.org/abs/2408.13040v1', content=\"Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.\", raw_content=None, score=None, date=None, metadata={'authors': ['Kai-Wei Chang', 'Haibin Wu', 'Yu-Kai Wang', 'Yuan-Kuei Wu', 'Hua Shen', 'Wei-Cheng Tseng', 'Iu-thing Kang', 'Shang-Wen Li', 'Hung-yi Lee'], 'published': '2024-08-23', 'categories': ['eess.AS', 'cs.AI', 'cs.CL', 'cs.LG'], 'primary_category': 'eess.AS', 'pdf_url': 'https://arxiv.org/pdf/2408.13040v1'}), SearchResult(title='Towards the Study of Morphological Processing of the Tangkhul Language', url='http://arxiv.org/abs/2006.16212v1', content='There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus.', raw_content=None, score=None, date=None, metadata={'authors': ['Mirinso Shadang', 'Navanath Saharia', 'Thoudam Doren Singh'], 'published': '2020-06-29', 'categories': ['cs.CL'], 'primary_category': 'cs.CL', 'pdf_url': 'https://arxiv.org/pdf/2006.16212v1'})]), SearchQueryResult(query='Comparative analysis of transformer architecture vs. recurrent neural networks', tool=<ResearchTool.ARXIV: 'arxiv'>, source_type=<SourceType.ARXIV: 'arxiv'>, timestamp=datetime.datetime(2025, 11, 17, 18, 15, 10, 706333), results=[SearchResult(title='Predicting concentration levels of air pollutants by transfer learning and recurrent neural network', url='http://arxiv.org/abs/2502.01654v1', content='Air pollution (AP) poses a great threat to human health, and people are paying more attention than ever to its prediction. Accurate prediction of AP helps people to plan for their outdoor activities and aids protecting human health. In this paper, long-short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS) in Macau. Additionally, meteorological data and data on the concentration of APS have been utilized. Moreover, in Macau, some air quality monitoring stations (AQMSs) have less observed data in quantity, and, at the same time, some AQMSs recorded less observed data of certain types of APS. Therefore, the transfer learning and pre-trained neural networks have been employed to assist AQMSs with less observed data to build a neural network with high prediction accuracy. The experimental sample covers a period longer than 12-year and includes daily measurements from several APS as well as other more classical meteorological values. Records from five stations, four out of them are AQMSs and the remaining one is an automatic weather station, have been prepared from the aforesaid period and eventually underwent to computational intelligence techniques to build and extract a prediction knowledge-based system. As shown by experimentation, LSTM RNNs initialized with transfer learning methods have higher prediction accuracy; it incurred shorter training time than randomly initialized recurrent neural networks.', raw_content=None, score=None, date=None, metadata={'authors': ['Iat Hang Fong', 'Tengyue Li', 'Simon Fong', 'Raymond K. Wong', 'Antonio J. Tallón-Ballesteros'], 'published': '2025-01-30', 'categories': ['cs.LG', 'cs.NE', 'physics.ao-ph'], 'primary_category': 'cs.LG', 'pdf_url': 'https://arxiv.org/pdf/2502.01654v1'}), SearchResult(title='Continual Learning for Recurrent Neural Networks: an Empirical Evaluation', url='http://arxiv.org/abs/2103.07492v4', content='Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.', raw_content=None, score=None, date=None, metadata={'authors': ['Andrea Cossu', 'Antonio Carta', 'Vincenzo Lomonaco', 'Davide Bacciu'], 'published': '2021-03-12', 'categories': ['cs.LG', 'cs.AI'], 'primary_category': 'cs.LG', 'pdf_url': 'https://arxiv.org/pdf/2103.07492v4'}), SearchResult(title='The Deep Arbitrary Polynomial Chaos Neural Network or how Deep Artificial Neural Networks could benefit from Data-Driven Homogeneous Chaos Theory', url='http://arxiv.org/abs/2306.14753v1', content='Artificial Intelligence and Machine learning have been widely used in various fields of mathematical computing, physical modeling, computational science, communication science, and stochastic analysis. Approaches based on Deep Artificial Neural Networks (DANN) are very popular in our days. Depending on the learning task, the exact form of DANNs is determined via their multi-layer architecture, activation functions and the so-called loss function. However, for a majority of deep learning approaches based on DANNs, the kernel structure of neural signal processing remains the same, where the node response is encoded as a linear superposition of neural activity, while the non-linearity is triggered by the activation functions. In the current paper, we suggest to analyze the neural signal processing in DANNs from the point of view of homogeneous chaos theory as known from polynomial chaos expansion (PCE). From the PCE perspective, the (linear) response on each node of a DANN could be seen as a $1^{st}$ degree multi-variate polynomial of single neurons from the previous layer, i.e. linear weighted sum of monomials. From this point of view, the conventional DANN structure relies implicitly (but erroneously) on a Gaussian distribution of neural signals. Additionally, this view revels that by design DANNs do not necessarily fulfill any orthogonality or orthonormality condition for a majority of data-driven applications. Therefore, the prevailing handling of neural signals in DANNs could lead to redundant representation as any neural signal could contain some partial information from other neural signals. To tackle that challenge, we suggest to employ the data-driven generalization of PCE theory known as arbitrary polynomial chaos (aPC) to construct a corresponding multi-variate orthonormal representations on each node of a DANN to obtain Deep arbitrary polynomial chaos neural networks.', raw_content=None, score=None, date=None, metadata={'authors': ['Sergey Oladyshkin', 'Timothy Praditia', 'Ilja Kröker', 'Farid Mohammadi', 'Wolfgang Nowak', 'Sebastian Otte'], 'published': '2023-06-26', 'categories': ['cs.NE', 'stat.ML'], 'primary_category': 'cs.NE', 'pdf_url': 'https://arxiv.org/pdf/2306.14753v1'}), SearchResult(title='LiteLSTM Architecture Based on Weights Sharing for Recurrent Neural Networks', url='http://arxiv.org/abs/2301.04794v1', content='Long short-term memory (LSTM) is one of the robust recurrent neural network architectures for learning sequential data. However, it requires considerable computational power to learn and implement both software and hardware aspects. This paper proposed a novel LiteLSTM architecture based on reducing the LSTM computation components via the weights sharing concept to reduce the overall architecture computation cost and maintain the architecture performance. The proposed LiteLSTM can be significant for processing large data where time-consuming is crucial while hardware resources are limited, such as the security of IoT devices and medical data processing. The proposed model was evaluated and tested empirically on three different datasets from the computer vision, cybersecurity, speech emotion recognition domains. The proposed LiteLSTM has comparable accuracy to the other state-of-the-art recurrent architecture while using a smaller computation budget.', raw_content=None, score=None, date=None, metadata={'authors': ['Nelly Elsayed', 'Zag ElSayed', 'Anthony S. Maida'], 'published': '2023-01-12', 'categories': ['cs.LG'], 'primary_category': 'cs.LG', 'pdf_url': 'https://arxiv.org/pdf/2301.04794v1'}), SearchResult(title='Effect of dilution in asymmetric recurrent neural networks', url='http://arxiv.org/abs/1805.03886v1', content=\"We study with numerical simulation the possible limit behaviors of synchronous discrete-time deterministic recurrent neural networks composed of N binary neurons as a function of a network's level of dilution and asymmetry. The network dilution measures the fraction of neuron couples that are connected, and the network asymmetry measures to what extent the underlying connectivity matrix is asymmetric. For each given neural network, we study the dynamical evolution of all the different initial conditions, thus characterizing the full dynamical landscape without imposing any learning rule. Because of the deterministic dynamics, each trajectory converges to an attractor, that can be either a fixed point or a limit cycle. These attractors form the set of all the possible limit behaviors of the neural network. For each network, we then determine the convergence times, the limit cycles' length, the number of attractors, and the sizes of the attractors' basin. We show that there are two network structures that maximize the number of possible limit behaviors. The first optimal network structure is fully-connected and symmetric. On the contrary, the second optimal network structure is highly sparse and asymmetric. The latter optimal is similar to what observed in different biological neuronal circuits. These observations lead us to hypothesize that independently from any given learning model, an efficient and effective biologic network that stores a number of limit behaviors close to its maximum capacity tends to develop a connectivity structure similar to one of the optimal networks we found.\", raw_content=None, score=None, date=None, metadata={'authors': ['Viola Folli', 'Giorgio Gosti', 'Marco Leonetti', 'Giancarlo Ruocco'], 'published': '2018-05-10', 'categories': ['cond-mat.dis-nn', 'cs.NE', 'q-bio.NC'], 'primary_category': 'cond-mat.dis-nn', 'pdf_url': 'https://arxiv.org/pdf/1805.03886v1'})]), SearchQueryResult(query='Comparative analysis of transformer architecture vs. recurrent neural networks', tool=<ResearchTool.TAVILY: 'tavily'>, source_type=<SourceType.WEB: 'web'>, timestamp=datetime.datetime(2025, 11, 17, 18, 15, 13, 874452), results=[SearchResult(title='Transformer vs RNN in NLP: A Comparative Analysis - Appinventiv', url='https://appinventiv.com/blog/transformer-vs-rnn/', content='The Transformer architecture NLP, introduced in the groundbreaking paper “Attention is All You Need” by Vaswani et al., has revolutionized the field of Natural Language Processing. Unlike previously dominant RNN-based models like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit), Transformers do not rely on recurrence or convolution, instead using a self-attention mechanism that allows for more efficient processing of sequential data. [...] A. Transformers and RNNs both handle sequential data but differ in their approach, efficiency, performance, and many other aspects. For instance, Transformers utilize a self-attention mechanism to evaluate the significance of every word in a sentence simultaneously, which lets them handle longer sequences more efficiently. [...] This “looking at everything at once” approach means transformers are more parallelizable than RNNs, which process data sequentially. This parallel processing capability gives natural language processing with Transformers a computational advantage and allows them to capture global dependencies effectively.', raw_content=None, score=0.99986124, date=None, metadata={}), SearchResult(title='A Comparative Study on Transformer vs RNN in Speech Applications', url='https://ieeexplore.ieee.org/document/9003750/', content='Transformer is a sequence-to-sequence (S2S) architecture originally proposed for neural machine translation (NMT)  that rapidly replaces recurrent neural networks (RNN) in natural language processing tasks. This paper provides intensive comparisons of its performance with that of RNN for speech applications; automatic speech recognition (ASR), speech translation (ST), and text-to-speech (TTS).\\n\\nMore Like This [...] succeed our exciting outcomes. [...] recurrent neural networks (RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS benchmarks. Our experiments revealed various training tips and significant performance benefits obtained with Transformer for each task including the surprising superiority of Transformer in 13/15 ASR benchmarks in comparison with RNN. We are preparing to release Kaldi-style reproducible recipes using open source and publicly available datasets for all the ASR, ST, and TTS tasks for the community to', raw_content=None, score=0.9995859, date=None, metadata={}), SearchResult(title='[PDF] A Comparative Study on Transformer Vs RNN in Speech Applications', url='https://merl.com/publications/docs/TR2019-158.pdf', content='S. M. Lakew, M. Cettolo, and M. Federico, “A comparison of transformer and recurrent neural networks on multilingual neural machine translation,” in Proceedings of the 27th In-ternational Conference on Computational Linguistics, 2018, pp. 641–652.\\n S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based sequence-to-sequence speech recognition with the trans-former in Mandarin Chinese,” in Proc. Interspeech, 2018, pp. 791–795. [...] We adopted the same architecture for Transformer in  (e = 12, d = 6, dff = 2048, dhead = 4, datt = 256) for every corpus ex-cept for the largest, LibriSpeech (dhead = 8, datt = 512). For RNN, we followed our existing best architecture conﬁgured on each corpus as in previous studies , . [...] of encoded features for the decoder network.', raw_content=None, score=0.9992792, date=None, metadata={}), SearchResult(title='[PDF] Comparative Analysis of CNN, RNN, LSTM, and Transformer ...', url='https://kuey.net/index.php/kuey/article/download/10364/7966/19317', content='1: Architecture diagrams for CNN, RNN, LSTM, and Transformer 4. Comparative Analysis Feature CNN RNN LSTM Transformer Data Type 2D Images Time Series Text, Speech Any Sequence Dependency Modeling Local Short-range Long-range Global Attention Training Parallelism High Low Low High Gradient Stability Stable Unstable Stable Stable Computational Cost Low Moderate High Very High Model Size Small Medium Large Very Large Application Domains Vision Speech Time series, NLP NLP, Vision, Multimodal Table [...] machines to learn hierarchical, complex representations of data. Among the most widely adopted architectures are Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformers. Each of these architectures offers unique capabilities and presents distinct trade-offs in performance, interpretability, and computational efficiency. This paper presents an in-depth comparative analysis of CNNs, RNNs, LSTMs, and Transformers. We explore [...] Transformers provide superior results in NLP and are now being applied to vision tasks (e.g., Vision Transformers). Emerging trends show that hybrid models—such as CNN-RNN combinations for video processing, or CNN-Transformer stacks for vision-language tasks—outperform single-architecture models in many applications. 7. Conclusion This comprehensive comparative analysis of CNNs, RNNs, LSTMs, and Transformers reveals significant insights into the capabilities, limitations, and performance', raw_content=None, score=0.9992792, date=None, metadata={}), SearchResult(title='RNN vs LSTM vs GRU vs Transformers - GeeksforGeeks', url='https://www.geeksforgeeks.org/deep-learning/rnn-vs-lstm-vs-gru-vs-transformers/', content='| Parameter | RNN (Recurrent Neural Network) | LSTM (Long Short-Term Memory) | GRU (Gated Recurrent Unit) | Transformers |\\n ---  --- \\n| Architecture | Simple structure with loops. | Memory cells with input, forget and output gates. | Combines input and forget gates into update gate; fewer parameters | Utilizes an attention-based mechanism without recurrence. | [...] Transformers solved these problems by using self-attention which processes the entire sequence at once. This allows transformers to capture long-range dependencies more effectively and train much faster. Unlike RNN-based models, transformers do not rely on sequential steps helps in making them highly scalable and suitable for larger datasets and more complex tasks.\\n\\n## RNN vs LSTM vs GRU vs Transformers', raw_content=None, score=0.99916387, date=None, metadata={})])]\n",
      "\n",
      "================================================================================\n",
      "Query: 'Overview of transformer architecture in artificial intelligence' | Tool: wikipedia | Source: wikipedia\n",
      "================================================================================\n",
      "\n",
      "[1] Generative artificial intelligence\n",
      "    URL: https://en.wikipedia.org/wiki/Generative_artificial_intelligence\n",
      "    Content: Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts. Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora....\n",
      "\n",
      "[2] Hallucination (artificial intelligence)\n",
      "    URL: https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)\n",
      "    Content: In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called bullshitting, confabulation, or delusion) is a response generated by AI that contains false or misleading information presented as fact. This term draws a loose analogy with human psychology, where a hallucination typically involves false percepts. However, there is a key difference: AI hallucination is associated with erroneously constructed responses (confabulation), rather than perceptual experiences.  For example, a chatbot powered by large language models (LLMs), like ChatGPT, may embed plausible-sounding random falsehoods within its generated content. Detecting and mitigating errors and hallucinations pose significant challenges for practical deployment and reliability of LLMs in high-stakes scenarios, such as chip design, supply chain logistics, and medical diagnostics....\n",
      "\n",
      "[3] Artificial intelligence\n",
      "    URL: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "    Content: Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics....\n",
      "\n",
      "[4] Transformer (deep learning)\n",
      "    URL: https://en.wikipedia.org/wiki/Transformer_(deep_learning)\n",
      "    Content: In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.  Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.  The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google....\n",
      "\n",
      "[5] Glossary of artificial intelligence\n",
      "    URL: https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence\n",
      "    Content: This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence (AI), its subdisciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, Glossary of machine vision, and Glossary of logic.   == A ==  A* search Pronounced \"A-star\". A graph traversal and pathfinding algorithm which is used in many fields of computer science due to its completeness, optimality, and optimal efficiency.  abductive logic programming (ALP) A high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: 'Overview of transformer architecture in artificial intelligence' | Tool: tavily | Source: web\n",
      "================================================================================\n",
      "\n",
      "[6] LLM Transformer Model Visually Explained\n",
      "    URL: https://poloclub.github.io/transformer-explainer/\n",
      "    Score: 0.86\n",
      "    Content: Transformer is a neural network architecture that has fundamentally changed the approach to Artificial Intelligence. Transformer was first introduced in the seminal paper \"Attention is All You Need\" in 2017 and has since become the go-to architecture for deep learning models, powering text-generative models like OpenAI's GPT, Meta's Llama, and Google's Gemini. Beyond text, Transformer is also applied in audio generation, image recognition, protein structure prediction, and even game playing,...\n",
      "\n",
      "[7] Transformer (deep learning) - Wikipedia\n",
      "    URL: https://en.wikipedia.org/wiki/Transformer_(deep_learning)\n",
      "    Score: 0.81\n",
      "    Content: In deep learning, the transformer is an artificial neural network architecture based on the multi-head attention \"Attention (machine learning)\") mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token \"Tokenization (lexical analysis)\") is then contextualized \"Contextualization (computer science)\") within the scope of the context window with other (unmasked) [...] The attention mechanism used in the transformer architecture are scaled dot-product attention \"Attention (machine learning)\") units. For each unit, the transformer model learns three weight matrices: the query weights      W  Q     {\\displaystyle W^{Q}}  {\\displaystyle W^{Q}}, the key weights      W  K     {\\displaystyle W^{K}}  {\\displaystyle W^{K}}, and the value weights      W  V     {\\displaystyle W^{V}}  {\\displaystyle W^{V}}. [...] As the transformer architecture natively consists of operations over numbers (matrix multiplications, dot products, activation functions) rather than over text, there must first be a mapping from any input text to some numerical representation. This happens in three steps....\n",
      "\n",
      "[8] What is a Transformer Model? | IBM\n",
      "    URL: https://www.ibm.com/think/topics/transformer-model\n",
      "    Score: 0.80\n",
      "    Content: The transformer model is a type of neural network architecture that excels at processing sequential data, most prominently associated with large language models (LLMs). Transformer models have also achieved elite performance in other fields of artificial intelligence (AI), such as computer vision, speech recognition and time series forecasting. [...] The central feature of transformer models is their self-attention mechanism, from which transformer models derive their impressive ability to detect the relationships (or dependencies) between each part of an input sequence. Unlike the RNN and CNN architectures that preceded it, the transformer architecture uses only attention layers and standard feedforward layers. [...] The transformer architecture was first described in the seminal 2017 paper “Attention is All You Need” by Vaswani and others, which is now considered a watershed moment in deep learning.        Originally introduced as an evolution of the recurrent neural network (RNN)-based sequence-to-sequence models used for machine translation, transformer-based models have since attained cutting-edge advancements across nearly every machine learning (ML) discipline....\n",
      "\n",
      "[9] What are Transformers in Artificial Intelligence? - Amazon AWS\n",
      "    URL: https://aws.amazon.com/what-is/transformers-in-artificial-intelligence/\n",
      "    Score: 0.79\n",
      "    Content: Transformers are a type of neural network architecture that transforms or changes an input sequence into an output sequence. They do this by learning context and tracking relationships between sequence components. For example, consider this input sequence: \"What is the color of the sky?\" The transformer model uses an internal mathematical representation that identifies the relevancy and relationship between the words color, sky, and blue. It uses that knowledge to generate the output: \"The sky [...] ## What are the components of transformer architecture?  Transformer neural network architecture has several software layers that work together to generate the final output. The following image shows the components of transformation architecture, as explained in the rest of this section.  ### Input embeddings [...] Vision transformers (ViT) repurpose the transformer architecture for image classification tasks. Instead of processing an image as a grid of pixels, they view image data as a sequence of fixed-size patches, similar to how words are treated in a sentence. Each patch is flattened, linearly embedded, and then processed sequentially by the standard transformer encoder. Positional embeddings are added to maintain spatial information. This usage of global self-attention enables the model to capture...\n",
      "\n",
      "[10] How Transformers Work: A Detailed Exploration of ... - DataCamp\n",
      "    URL: https://www.datacamp.com/tutorial/how-transformers-work\n",
      "    Score: 0.78\n",
      "    Content: First appeared in 2017 in the “Attention is all you need” article by Google, the transformer architecture is at the heart of groundbreaking models like ChatGPT, sparking a new wave of excitement in the AI community. They've been instrumental in OpenAI's cutting-edge language models and played a key role in DeepMind's AlphaStar.  In this transformative era of AI, the significance of Transformer models for aspiring data scientists and NLP practitioners cannot be overstated. [...] A transformer is a type of artificial intelligence model that learns to understand and generate human-like text by analyzing patterns in large amounts of text data.  Transformers are a current state-of-the-art NLP model and are considered the evolution of the encoder-decoder architecture. However, while the encoder-decoder architecture relies mainly on Recurrent Neural Networks (RNNs) to extract sequential information, Transformers completely lack this recurrency.  So, how do they do it? [...] The shift from Recurrent Neural Networks (RNNs) like LSTM to Transformers in NLP is driven by these two main problems and Transformers' ability to assess both of them by taking advantage of the Attention mechanism improvements:   Pay attention to specific words, no matter how distant they are.  Boost the performance speed.  Thus, Transformers became a natural improvement of RNNs.  Next, let’s take a look at how transformers work.  ## The Transformer Architecture  ### Overview...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: 'Recent advancements in transformer models for natural language processing' | Tool: tavily | Source: web\n",
      "================================================================================\n",
      "\n",
      "[11] Transformer Models in Natural Language Processing - Netguru\n",
      "    URL: https://www.netguru.com/blog/transformer-models-in-nlp\n",
      "    Score: 1.00\n",
      "    Content: Research collaborations between academia and industry have yielded specialized transformers for speech recognition that achieve word error rates below 5% in many languages. These developments often emerge from collaborative efforts spanning multiple institutions. [...] Natural Language Processing (NLP) represents the original and most mature application of transformer models. When introduced in 2017, transformers quickly surpassed previous approaches to machine translation, establishing new benchmarks in accuracy and fluency.  The self-attention mechanism allows transformers to grasp context and relationships between words in a sentence, regardless of their distance from each other. This capability proves crucial for tasks like: [...] The key innovation was self-attention, allowing the model to weigh the importance of words against each other regardless of their position in the text. This enabled truly parallel processing, making training much faster and more efficient.  The original Transformer quickly led to powerful models like BERT (2018) and GPT (2018), which demonstrated unprecedented abilities in understanding and generating human language. These models scaled effectively with more data and larger parameter counts....\n",
      "\n",
      "[12] [PDF] Advanced Transformer-Based Deep Learning Techniques For ...\n",
      "    URL: https://africanjournalofbiomedicalresearch.com/index.php/AJBR/article/download/4562/3507/8591\n",
      "    Score: 1.00\n",
      "    Content: Enhancing Contextual Understanding In Natural Language Processing 5155 Afr. J. Biomed. Res. Vol. 27, No.4s (December) 2024 Dr M Nithin Varma et al Additionally, reversible layers and memory-efficient encoding strategies have been proposed to reduce the computational burden during training and inference. These developments highlight the ongoing efforts to make transformer models more scalable and efficient while preserving their contextual capabilities. The integration of external knowledge has [...] Accuracy 78.00% 85.00% 9.00% Advanced Transformer-Based Deep Learning Techniques For Enhancing Contextual Understanding In Natural Language Processing 5160 Afr. J. Biomed. Res. Vol. 27, No.4s (December) 2024 Dr M Nithin Varma et al Figure 6: Performance Trends Across Tasks The findings confirm that the proposed transformer-based model addresses key limitations of existing architectures, demonstrating superior contextual understanding, computational efficiency, and adaptability. These results [...] and dynamic encoding strategies(Murali Krishnam Raju et al., 2022). This study aims to bridge these gaps by proposing a novel transformer-based architecture that incorporates these advancements to enhance performance and contextual understanding across diverse NLP tasks. 3. Methodology This section outlines the systematic approach adopted for designing, training, and evaluating the proposed advanced transformer-based model. Key components include architectural enhancements, data preparation,...\n",
      "\n",
      "[13] The Transformer Model: Revolutionizing Natural Language ...\n",
      "    URL: https://medium.com/@lordmoma/the-transformer-model-revolutionizing-natural-language-processing-a16be54ddb1e\n",
      "    Score: 1.00\n",
      "    Content: The Transformer model is a powerful deep learning architecture that has revolutionized the field of natural language processing. Its ability to handle long-range dependencies in text data makes it well-suited for a wide range of NLP tasks. The tokenizer, embedding, positional encoding, Transformer block, attention, feedforward, and softmax components work together to enable the model to guess the next word in a text sequence with remarkable accuracy. With continued advances in deep learning [...] research, the Transformer model is poised to remain at the forefront of NLP for years to come. [...] Sign up  Sign in  Sign up  Sign in  # The Transformer Model: Revolutionizing Natural Language Processing  David Lee  --  Listen  Share  The Transformer model is a deep learning architecture that has revolutionized the field of natural language processing (NLP). It is based on a self-attention mechanism that allows it to handle long-range dependencies in text data, making it well-suited for tasks such as language translation, summarization, and question-answering....\n",
      "\n",
      "[14] What is a Transformer Model? | IBM\n",
      "    URL: https://www.ibm.com/think/topics/transformer-model\n",
      "    Score: 1.00\n",
      "    Content: Transformer models are most commonly associated with NLP, having originally been developed for machine translation use cases. Most notably, the transformer architecture gave rise to the large language models (LLMs) that catalyzed the advent of generative AI.  Most of the LLMs that the public is most familiar with, from closed source models such as OpenAI’s GPT series and Anthropic’s Claude models to open source models including Meta Llama or IBM® Granite®, are autoregressive decoder-only LLMs. [...] The transformer model is a type of neural network architecture that excels at processing sequential data, most prominently associated with large language models (LLMs). Transformer models have also achieved elite performance in other fields of artificial intelligence (AI), such as computer vision, speech recognition and time series forecasting. [...] ## Transformer models in other fields  Though transformer models were originally designed for, and continue to be most prominently associated with natural language use cases, they can be used in nearly any situation involving sequential data. This has led to the development of transformer-based models in other fields, from fine-tuning LLMs into multimodal systems to dedicated time series forecasting models and ViTs for computer vision....\n",
      "\n",
      "[15] Transformers: State-of-the-Art Natural Language Processing\n",
      "    URL: https://aclanthology.org/2020.emnlp-demos.6/\n",
      "    Score: 0.99\n",
      "    Content: Transformers: State-of-the-Art Natural Language Processing (Wolf et al., EMNLP 2020)  ##### ACL [...] ACL Logo  ## Transformers: State-of-the-Art Natural Language Processing  Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander Rush  ##### Correct Metadata for  ##### Abstract  ##### Export citation  ##### Markdown (Informal) [...] The ACL Anthology is managed and built by the ACL Anthology team of volunteers.  Site last built on 14 November 2025 at 22:36 UTC with commit b62994a....\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: 'Recent advancements in transformer models for natural language processing' | Tool: arxiv | Source: arxiv\n",
      "================================================================================\n",
      "\n",
      "[16] An Open Natural Language Processing Development Framework for EHR-based Clinical Research: A case demonstration using the National COVID Cohort Collaborative (N3C)\n",
      "    URL: http://arxiv.org/abs/2110.10780v3\n",
      "    Content: While we pay attention to the latest advances in clinical natural language processing (NLP), we can notice some resistance in the clinical and translational research community to adopt NLP models due to limited transparency, interpretability, and usability. In this study, we proposed an open natural language processing development framework. We evaluated it through the implementation of NLP algorithms for the National COVID Cohort Collaborative (N3C). Based on the interests in information extraction from COVID-19 related clinical notes, our work includes 1) an open data annotation process using COVID-19 signs and symptoms as the use case, 2) a community-driven ruleset composing platform, and 3) a synthetic text data generation workflow to generate texts for information extraction tasks without involving human subjects. The corpora were derived from texts from three different institutions (Mayo Clinic, University of Kentucky, University of Minnesota). The gold standard annotations were tested with a single institution's (Mayo) ruleset. This resulted in performances of 0.876, 0.706, and 0.694 in F-scores for Mayo, Minnesota, and Kentucky test datasets, respectively. The study as a consortium effort of the N3C NLP subgroup demonstrates the feasibility of creating a federated NLP algorithm development and benchmarking platform to enhance multi-institution clinical NLP study and adoption. Although we use COVID-19 as a use case in this effort, our framework is general enough to be applied to other domains of interest in clinical NLP....\n",
      "    Metadata: Authors: Sijia Liu, Andrew Wen | Published: 2021-10-20\n",
      "\n",
      "[17] A Review of Bangla Natural Language Processing Tasks and the Utility of Transformer Models\n",
      "    URL: http://arxiv.org/abs/2107.03844v3\n",
      "    Content: Bangla -- ranked as the 6th most widely spoken language across the world (https://www.ethnologue.com/guides/ethnologue200), with 230 million native speakers -- is still considered as a low-resource language in the natural language processing (NLP) community. With three decades of research, Bangla NLP (BNLP) is still lagging behind mainly due to the scarcity of resources and the challenges that come with it. There is sparse work in different areas of BNLP; however, a thorough survey reporting previous work and recent advances is yet to be done. In this study, we first provide a review of Bangla NLP tasks, resources, and tools available to the research community; we benchmark datasets collected from various platforms for nine NLP tasks using current state-of-the-art algorithms (i.e., transformer-based models). We provide comparative results for the studied NLP tasks by comparing monolingual vs. multilingual models of varying sizes. We report our results using both individual and consolidated datasets and provide data splits for future research. We reviewed a total of 108 papers and conducted 175 sets of experiments. Our results show promising performance using transformer-based models while highlighting the trade-off with computational costs. We hope that such a comprehensive survey will motivate the community to build on and further advance the research on Bangla NLP....\n",
      "    Metadata: Authors: Firoj Alam, Arid Hasan | Published: 2021-07-08\n",
      "\n",
      "[18] A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text\n",
      "    URL: http://arxiv.org/abs/2306.06371v1\n",
      "    Content: Java Code Generation consists in generating automatically Java code from a Natural Language Text. This NLP task helps in increasing programmers' productivity by providing them with immediate solutions to the simplest and most repetitive tasks. Code generation is a challenging task because of the hard syntactic rules and the necessity of a deep understanding of the semantic aspect of the programming language. Many works tried to tackle this task using either RNN-based, or Transformer-based models. The latter achieved remarkable advancement in the domain and they can be divided into three groups: (1) encoder-only models, (2) decoder-only models, and (3) encoder-decoder models. In this paper, we provide a comprehensive review of the evolution and progress of deep learning models in Java code generation task. We focus on the most important methods and present their merits and limitations, as well as the objective functions used by the community. In addition, we provide a detailed description of datasets and evaluation metrics used in the literature. Finally, we discuss results of different models on CONCODE dataset, then propose some future directions....\n",
      "    Metadata: Authors: Jessica López Espejel, Mahaman Sanoussi Yahaya Alassan | Published: 2023-06-10\n",
      "\n",
      "[19] SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks\n",
      "    URL: http://arxiv.org/abs/2408.13040v1\n",
      "    Content: Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential....\n",
      "    Metadata: Authors: Kai-Wei Chang, Haibin Wu | Published: 2024-08-23\n",
      "\n",
      "[20] Towards the Study of Morphological Processing of the Tangkhul Language\n",
      "    URL: http://arxiv.org/abs/2006.16212v1\n",
      "    Content: There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus....\n",
      "    Metadata: Authors: Mirinso Shadang, Navanath Saharia | Published: 2020-06-29\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: 'Comparative analysis of transformer architecture vs. recurrent neural networks' | Tool: arxiv | Source: arxiv\n",
      "================================================================================\n",
      "\n",
      "[21] Predicting concentration levels of air pollutants by transfer learning and recurrent neural network\n",
      "    URL: http://arxiv.org/abs/2502.01654v1\n",
      "    Content: Air pollution (AP) poses a great threat to human health, and people are paying more attention than ever to its prediction. Accurate prediction of AP helps people to plan for their outdoor activities and aids protecting human health. In this paper, long-short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS) in Macau. Additionally, meteorological data and data on the concentration of APS have been utilized. Moreover, in Macau, some air quality monitoring stations (AQMSs) have less observed data in quantity, and, at the same time, some AQMSs recorded less observed data of certain types of APS. Therefore, the transfer learning and pre-trained neural networks have been employed to assist AQMSs with less observed data to build a neural network with high prediction accuracy. The experimental sample covers a period longer than 12-year and includes daily measurements from several APS as well as other more classical meteorological values. Records from five stations, four out of them are AQMSs and the remaining one is an automatic weather station, have been prepared from the aforesaid period and eventually underwent to computational intelligence techniques to build and extract a prediction knowledge-based system. As shown by experimentation, LSTM RNNs initialized with transfer learning methods have higher prediction accuracy; it incurred shorter training time than randomly initialized recurrent neural networks....\n",
      "    Metadata: Authors: Iat Hang Fong, Tengyue Li | Published: 2025-01-30\n",
      "\n",
      "[22] Continual Learning for Recurrent Neural Networks: an Empirical Evaluation\n",
      "    URL: http://arxiv.org/abs/2103.07492v4\n",
      "    Content: Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario....\n",
      "    Metadata: Authors: Andrea Cossu, Antonio Carta | Published: 2021-03-12\n",
      "\n",
      "[23] The Deep Arbitrary Polynomial Chaos Neural Network or how Deep Artificial Neural Networks could benefit from Data-Driven Homogeneous Chaos Theory\n",
      "    URL: http://arxiv.org/abs/2306.14753v1\n",
      "    Content: Artificial Intelligence and Machine learning have been widely used in various fields of mathematical computing, physical modeling, computational science, communication science, and stochastic analysis. Approaches based on Deep Artificial Neural Networks (DANN) are very popular in our days. Depending on the learning task, the exact form of DANNs is determined via their multi-layer architecture, activation functions and the so-called loss function. However, for a majority of deep learning approaches based on DANNs, the kernel structure of neural signal processing remains the same, where the node response is encoded as a linear superposition of neural activity, while the non-linearity is triggered by the activation functions. In the current paper, we suggest to analyze the neural signal processing in DANNs from the point of view of homogeneous chaos theory as known from polynomial chaos expansion (PCE). From the PCE perspective, the (linear) response on each node of a DANN could be seen as a $1^{st}$ degree multi-variate polynomial of single neurons from the previous layer, i.e. linear weighted sum of monomials. From this point of view, the conventional DANN structure relies implicitly (but erroneously) on a Gaussian distribution of neural signals. Additionally, this view revels that by design DANNs do not necessarily fulfill any orthogonality or orthonormality condition for a majority of data-driven applications. Therefore, the prevailing handling of neural signals in DANNs could lead to redundant representation as any neural signal could contain some partial information from other neural signals. To tackle that challenge, we suggest to employ the data-driven generalization of PCE theory known as arbitrary polynomial chaos (aPC) to construct a corresponding multi-variate orthonormal representations on each node of a DANN to obtain Deep arbitrary polynomial chaos neural networks....\n",
      "    Metadata: Authors: Sergey Oladyshkin, Timothy Praditia | Published: 2023-06-26\n",
      "\n",
      "[24] LiteLSTM Architecture Based on Weights Sharing for Recurrent Neural Networks\n",
      "    URL: http://arxiv.org/abs/2301.04794v1\n",
      "    Content: Long short-term memory (LSTM) is one of the robust recurrent neural network architectures for learning sequential data. However, it requires considerable computational power to learn and implement both software and hardware aspects. This paper proposed a novel LiteLSTM architecture based on reducing the LSTM computation components via the weights sharing concept to reduce the overall architecture computation cost and maintain the architecture performance. The proposed LiteLSTM can be significant for processing large data where time-consuming is crucial while hardware resources are limited, such as the security of IoT devices and medical data processing. The proposed model was evaluated and tested empirically on three different datasets from the computer vision, cybersecurity, speech emotion recognition domains. The proposed LiteLSTM has comparable accuracy to the other state-of-the-art recurrent architecture while using a smaller computation budget....\n",
      "    Metadata: Authors: Nelly Elsayed, Zag ElSayed | Published: 2023-01-12\n",
      "\n",
      "[25] Effect of dilution in asymmetric recurrent neural networks\n",
      "    URL: http://arxiv.org/abs/1805.03886v1\n",
      "    Content: We study with numerical simulation the possible limit behaviors of synchronous discrete-time deterministic recurrent neural networks composed of N binary neurons as a function of a network's level of dilution and asymmetry. The network dilution measures the fraction of neuron couples that are connected, and the network asymmetry measures to what extent the underlying connectivity matrix is asymmetric. For each given neural network, we study the dynamical evolution of all the different initial conditions, thus characterizing the full dynamical landscape without imposing any learning rule. Because of the deterministic dynamics, each trajectory converges to an attractor, that can be either a fixed point or a limit cycle. These attractors form the set of all the possible limit behaviors of the neural network. For each network, we then determine the convergence times, the limit cycles' length, the number of attractors, and the sizes of the attractors' basin. We show that there are two network structures that maximize the number of possible limit behaviors. The first optimal network structure is fully-connected and symmetric. On the contrary, the second optimal network structure is highly sparse and asymmetric. The latter optimal is similar to what observed in different biological neuronal circuits. These observations lead us to hypothesize that independently from any given learning model, an efficient and effective biologic network that stores a number of limit behaviors close to its maximum capacity tends to develop a connectivity structure similar to one of the optimal networks we found....\n",
      "    Metadata: Authors: Viola Folli, Giorgio Gosti | Published: 2018-05-10\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query: 'Comparative analysis of transformer architecture vs. recurrent neural networks' | Tool: tavily | Source: web\n",
      "================================================================================\n",
      "\n",
      "[26] Transformer vs RNN in NLP: A Comparative Analysis - Appinventiv\n",
      "    URL: https://appinventiv.com/blog/transformer-vs-rnn/\n",
      "    Score: 1.00\n",
      "    Content: The Transformer architecture NLP, introduced in the groundbreaking paper “Attention is All You Need” by Vaswani et al., has revolutionized the field of Natural Language Processing. Unlike previously dominant RNN-based models like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit), Transformers do not rely on recurrence or convolution, instead using a self-attention mechanism that allows for more efficient processing of sequential data. [...] A. Transformers and RNNs both handle sequential data but differ in their approach, efficiency, performance, and many other aspects. For instance, Transformers utilize a self-attention mechanism to evaluate the significance of every word in a sentence simultaneously, which lets them handle longer sequences more efficiently. [...] This “looking at everything at once” approach means transformers are more parallelizable than RNNs, which process data sequentially. This parallel processing capability gives natural language processing with Transformers a computational advantage and allows them to capture global dependencies effectively....\n",
      "\n",
      "[27] A Comparative Study on Transformer vs RNN in Speech Applications\n",
      "    URL: https://ieeexplore.ieee.org/document/9003750/\n",
      "    Score: 1.00\n",
      "    Content: Transformer is a sequence-to-sequence (S2S) architecture originally proposed for neural machine translation (NMT)  that rapidly replaces recurrent neural networks (RNN) in natural language processing tasks. This paper provides intensive comparisons of its performance with that of RNN for speech applications; automatic speech recognition (ASR), speech translation (ST), and text-to-speech (TTS).  More Like This [...] succeed our exciting outcomes. [...] recurrent neural networks (RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS benchmarks. Our experiments revealed various training tips and significant performance benefits obtained with Transformer for each task including the surprising superiority of Transformer in 13/15 ASR benchmarks in comparison with RNN. We are preparing to release Kaldi-style reproducible recipes using open source and publicly available datasets for all the ASR, ST, and TTS tasks for the community to...\n",
      "\n",
      "[28] [PDF] A Comparative Study on Transformer Vs RNN in Speech Applications\n",
      "    URL: https://merl.com/publications/docs/TR2019-158.pdf\n",
      "    Score: 1.00\n",
      "    Content: S. M. Lakew, M. Cettolo, and M. Federico, “A comparison of transformer and recurrent neural networks on multilingual neural machine translation,” in Proceedings of the 27th In-ternational Conference on Computational Linguistics, 2018, pp. 641–652.  S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based sequence-to-sequence speech recognition with the trans-former in Mandarin Chinese,” in Proc. Interspeech, 2018, pp. 791–795. [...] We adopted the same architecture for Transformer in  (e = 12, d = 6, dff = 2048, dhead = 4, datt = 256) for every corpus ex-cept for the largest, LibriSpeech (dhead = 8, datt = 512). For RNN, we followed our existing best architecture conﬁgured on each corpus as in previous studies , . [...] of encoded features for the decoder network....\n",
      "\n",
      "[29] [PDF] Comparative Analysis of CNN, RNN, LSTM, and Transformer ...\n",
      "    URL: https://kuey.net/index.php/kuey/article/download/10364/7966/19317\n",
      "    Score: 1.00\n",
      "    Content: 1: Architecture diagrams for CNN, RNN, LSTM, and Transformer 4. Comparative Analysis Feature CNN RNN LSTM Transformer Data Type 2D Images Time Series Text, Speech Any Sequence Dependency Modeling Local Short-range Long-range Global Attention Training Parallelism High Low Low High Gradient Stability Stable Unstable Stable Stable Computational Cost Low Moderate High Very High Model Size Small Medium Large Very Large Application Domains Vision Speech Time series, NLP NLP, Vision, Multimodal Table [...] machines to learn hierarchical, complex representations of data. Among the most widely adopted architectures are Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformers. Each of these architectures offers unique capabilities and presents distinct trade-offs in performance, interpretability, and computational efficiency. This paper presents an in-depth comparative analysis of CNNs, RNNs, LSTMs, and Transformers. We explore [...] Transformers provide superior results in NLP and are now being applied to vision tasks (e.g., Vision Transformers). Emerging trends show that hybrid models—such as CNN-RNN combinations for video processing, or CNN-Transformer stacks for vision-language tasks—outperform single-architecture models in many applications. 7. Conclusion This comprehensive comparative analysis of CNNs, RNNs, LSTMs, and Transformers reveals significant insights into the capabilities, limitations, and performance...\n",
      "\n",
      "[30] RNN vs LSTM vs GRU vs Transformers - GeeksforGeeks\n",
      "    URL: https://www.geeksforgeeks.org/deep-learning/rnn-vs-lstm-vs-gru-vs-transformers/\n",
      "    Score: 1.00\n",
      "    Content: | Parameter | RNN (Recurrent Neural Network) | LSTM (Long Short-Term Memory) | GRU (Gated Recurrent Unit) | Transformers |  ---  ---  | Architecture | Simple structure with loops. | Memory cells with input, forget and output gates. | Combines input and forget gates into update gate; fewer parameters | Utilizes an attention-based mechanism without recurrence. | [...] Transformers solved these problems by using self-attention which processes the entire sequence at once. This allows transformers to capture long-range dependencies more effectively and train much faster. Unlike RNN-based models, transformers do not rely on sequential steps helps in making them highly scalable and suitable for larger datasets and more complex tasks.  ## RNN vs LSTM vs GRU vs Transformers...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from core.graph import format_search_results\n",
    "print(all_results)\n",
    "formatted_results = format_search_results(all_results)\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06992d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web-research (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
