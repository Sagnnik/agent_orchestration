{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309c4598",
   "metadata": {},
   "source": [
    "## Getting the reasoning logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b0d0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Quantum physics is notoriously difficult to grasp for several interconnected reasons, rooted in its abstract concepts, mathematical complexity, and departure from classical intuition. Here's a breakdown of the key factors:\n",
      "\n",
      "### 1. **Counterintuitive Nature of Reality**  \n",
      "   - **Wave-Particle Duality**: Particles like electrons exhibit both wave-like and particle-like behavior, which defies classical logic. For example, light behaves as a wave in some experiments and as a particle (photon) in others.  \n",
      "   - **Superposition**: Quantum systems can exist in multiple states simultaneously (e.g., a particle being in two places at once) until measured. This is fundamentally different from our everyday experience.  \n",
      "   - **Entanglement**: Particles can become \"spooky\" connected, with their states correlated instantaneously across vast distances, even if separated by space. This challenges classical notions of locality and causality.  \n",
      "   - **Uncertainty Principle**: Certain pairs of properties (like position and momentum) cannot be simultaneously measured with precision. This inherent randomness contrasts with the deterministic predictions of classical physics.\n",
      "\n",
      "### 2. **Mathematical Complexity**  \n",
      "   - **Abstract Formalism**: Quantum mechanics relies on advanced mathematics, such as Hilbert spaces, operators, and complex numbers, which are not intuitive for most people.  \n",
      "   - **Wave Functions and Operators**: The state of a quantum system is described by a wave function (a mathematical object in a complex space), and physical quantities are represented by operators. This abstraction is far removed from classical equations.  \n",
      "   - **Schrödinger Equation**: Solving the Schrödinger equation for complex systems (e.g., molecules) requires advanced techniques and often cannot be done analytically, relying instead on approximations or computational methods.\n",
      "\n",
      "### 3. **Probabilistic Framework**  \n",
      "   - **Probabilistic Outcomes**: Unlike classical physics, which predicts exact results, quantum mechanics only provides probabilities for outcomes. This probabilistic nature is not just a tool but a fundamental aspect of the theory.  \n",
      "   - **Measurement Problem**: The act of measurement collapses the wave function into a definite state, but the exact mechanism of this collapse remains debated (e.g., the Copenhagen interpretation vs. many-worlds theory).\n",
      "\n",
      "### 4. **Historical and Conceptual Challenges**  \n",
      "   - **Revolutionary Foundations**: Quantum mechanics emerged in the early 20th century as a radical departure from classical physics. Its principles (e.g., quantization of energy, wave-particle duality) were initially met with skepticism and required rethinking of physical laws.  \n",
      "   - **Multiple Interpretations**: There is no consensus on the \"correct\" interpretation of quantum mechanics (e.g., Copenhagen, many-worlds, Bohmian mechanics). This lack of a unified framework can confuse students and researchers alike.\n",
      "\n",
      "### 5. **Lack of Direct Intuition**  \n",
      "   - **No Macroscopic Analogies**: Quantum phenomena (e.g., tunneling, quantum coherence) have no direct counterparts in the macroscopic world. This makes it hard to build intuitive models.  \n",
      "   - **Non-Local Correlations**: Entanglement defies classical ideas of locality, making it difficult to visualize or reconcile with everyday experiences.\n",
      "\n",
      "### 6. **Interconnected Concepts**  \n",
      "   - **Holistic Understanding**: Quantum mechanics requires understanding interrelated concepts (e.g., wave functions, operators, eigenvalues, and measurement postulates). Missing one can make the entire framework seem incoherent.\n",
      "\n",
      "### 7. **Educational Barriers**  \n",
      "   - **Mathematical Rigor**: Many introductory courses emphasize mathematical formalism over conceptual understanding, which can overwhelm students unfamiliar with advanced math.  \n",
      "   - **Historical Context**: The theory’s development was driven by experimental anomalies (e.g., blackbody radiation, double-slit experiments), which are often underemphasized in favor of equations.\n",
      "\n",
      "### Conclusion  \n",
      "Quantum physics is hard because it requires rethinking fundamental assumptions about reality, mastering complex mathematics, and grappling with abstract, non-intuitive concepts. Its success lies in its ability to describe the universe at the smallest scales, even if it challenges our classical intuitions. Overcoming these difficulties often involves a combination of deep conceptual understanding, mathematical proficiency, and exposure to experimental evidence. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "model = ChatOllama(model=\"qwen3:8b\") # i can also pass reasoning=True here\n",
    "msg = [HumanMessage(\"why is quantum physics so hard?\")]\n",
    "\n",
    "response = model.invoke(msg, reasoning=True)\n",
    "print(\"Answer:\\n\", response.content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7abd961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking / reasoning trace:\n",
      " Okay, the user is asking why quantum physics is so hard. Let me start by breaking down the main reasons. First, quantum mechanics deals with particles at a very small scale, which is different from the macroscopic world we're used to. So, the concepts like superposition and entanglement don't align with our everyday experiences. That's probably a big part of the difficulty.\n",
      "\n",
      "Then there's the mathematical complexity. The equations in quantum mechanics, like the Schrödinger equation, are more advanced than classical physics. Students might struggle with the abstract mathematics involved, especially if they're not comfortable with linear algebra or differential equations.\n",
      "\n",
      "Another point is the counterintuitive nature of the theory. Things like wave-particle duality and the uncertainty principle challenge our classical intuitions. People might find it hard to grasp because these concepts don't fit with how we perceive reality. For example, the idea that a particle can be in multiple states at once until measured is really foreign.\n",
      "\n",
      "Also, the interpretation of quantum mechanics itself is a source of confusion. There are multiple interpretations, like the Copenhagen interpretation, many-worlds, and others. Each offers different ways to understand the theory, and without a clear consensus, it can be hard for students to know which to accept.\n",
      "\n",
      "Then there's the historical context. Quantum physics was developed in the early 20th century, and the foundational theories were built on a lot of experimental results that contradicted classical physics. This means the theory is based on observations that don't align with classical logic, making it harder to intuitively understand.\n",
      "\n",
      "Additionally, the mathematical formalism is quite abstract. Unlike classical mechanics, which uses Newtonian equations, quantum mechanics uses operators and Hilbert spaces. This abstraction can be a barrier for students who are more used to concrete, visual models.\n",
      "\n",
      "The lack of direct observable phenomena also plays a role. In quantum mechanics, many effects are probabilistic and not directly measurable in the same way as classical systems. This probabilistic nature is a departure from deterministic physics, which might be confusing for those used to predictable outcomes.\n",
      "\n",
      "Moreover, the interconnectedness of concepts in quantum mechanics means that understanding one part often requires knowledge of many others. For example, wave functions, operators, eigenvalues, and measurement postulates are all linked, and missing one can make the whole subject seem incoherent.\n",
      "\n",
      "I should also mention that quantum physics is a highly mathematical field, which can be daunting for those without a strong background in mathematics. The transition from classical to quantum mechanics requires not just learning new concepts but also mastering new mathematical tools.\n",
      "\n",
      "Finally, the educational approach might contribute to the difficulty. Sometimes, the material is presented in a way that emphasizes the mathematical formalism over the conceptual understanding, which can make it hard for students to see the bigger picture.\n",
      "\n",
      "Putting this all together, the combination of abstract concepts, complex mathematics, counterintuitive principles, and the lack of direct observable analogies makes quantum physics challenging. It's not just about memorizing formulas but truly grasping a different way of understanding the universe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Thinking / reasoning trace:\\n\", response.additional_kwargs.get(\"reasoning_content\") or \"No reasoning field found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.messages import HumanMessage, ToolMessage, AIMessage\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "#get the model\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "model.invoke(\"tell me a joke\")\n",
    "\n",
    "#define the tool\n",
    "search_tool = TavilySearch()\n",
    "\n",
    "messages = [HumanMessage(\"add 35 + 156\")]\n",
    "#call the model by binding the tool\n",
    "tool_llm = model.bind_tools(tools=[search_tool])\n",
    "response = tool_llm.invoke(messages)\n",
    "print(\"Tool call: \",response.tool_calls)\n",
    "# manually call the tool and append the ToolMessage\n",
    "for tc in response.tool_calls:\n",
    "    name = tc[\"name\"]\n",
    "    args = tc['args']\n",
    "    result = tool_llm.invoke(args)\n",
    "    messages += [response, ToolMessage(content=str(result), tool_call_id=tc['id'])]\n",
    "\n",
    "final = tool_llm.invoke(messages)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d57df7",
   "metadata": {},
   "source": [
    "## Testing search tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da354f67",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf35677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visited: ['Graph neural network', '15.ai', 'AAAI Conference on Artificial Intelligence', 'AI alignment', 'AI safety', 'Action selection', 'Activation function', 'Active learning (machine learning)', 'Adjacency matrix', 'Adobe Firefly', 'Adversarial machine learning', '/mlp/', 'NetEase', '2001: A Space Odyssey', '44,100 Hz', '4chan', 'Ethics of artificial intelligence', 'Partnership on AI', 'AI boom', 'Academic conference', 'Alberta', 'Amazon (company)', 'Anaheim, California', 'Anaheim Convention Center', 'Artificial intelligence', 'Association for the Advancement of Artificial Intelligence', 'Atlanta', 'Austin, Texas', 'Baidu', 'AI-assisted software development', 'AI bubble', 'AI capability control', 'Artificial intelligence industry in China', 'AI takeover', 'AI winter', 'Alan Turing', 'AI21 Labs', 'AI Safety Institute', 'AI Safety Summit', 'AI Seoul Summit']\n",
      "TITLE: Graph neural network\n",
      "URL: https://en.wikipedia.org/wiki/Graph_neural_network\n",
      "SUMMARY (first 300 chars): Graph neural networks (GNN) are specialized artificial neural networks that are designed for tasks whose inputs are graphs.\n",
      "One prominent example is molecular drug design.  Each input sample is a graph representation of a molecule, where atoms form the nodes and chemical bonds between atoms form the\n",
      "LINKS (first 10): ['15.ai', 'AAAI Conference on Artificial Intelligence', 'AI alignment', 'AI safety', 'Action selection', 'Activation function', 'Active learning (machine learning)', 'Adjacency matrix', 'Adobe Firefly', 'Adversarial attack']\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, List, Set, Tuple, Optional\n",
    "import time\n",
    "import wikipedia\n",
    "from wikipedia.exceptions import DisambiguationError, PageError\n",
    "\n",
    "class WikipediaScraper:\n",
    "    \"\"\"\n",
    "    Simple Wikipedia scraper using the 'wikipedia' package.\n",
    "    Returns page metadata + content + a link-edge list.\n",
    "    \"\"\"\n",
    "\n",
    "    DEPTH_CONFIG = {\n",
    "        # (max_hops, max_pages_total, max_links_per_page)\n",
    "        \"shallow\":  (0,  1,  20),  # only the main page\n",
    "        \"moderate\": (1, 10,  15),  # main page + follow top links (1 hop)\n",
    "        \"deep\":     (2, 50,  10),  # up to 2 hops and more pages allowed\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_agent: str = \"LangGraph-WikiScraper\",\n",
    "        delay_between_requests: float = 0.5,\n",
    "    ):\n",
    "\n",
    "        wikipedia.set_lang(\"en\")\n",
    "        self.delay = delay_between_requests\n",
    "        self.user_agent = user_agent\n",
    "\n",
    "    def _safe_get_page(self, title: str) -> Optional[wikipedia.WikipediaPage]:\n",
    "        \"\"\"\n",
    "        Try to fetch a page object for title. Handles disambiguation and page errors.\n",
    "        Returns None on failure.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # This may raise DisambiguationError or PageError\n",
    "            page = wikipedia.page(title, auto_suggest=False, preload=False)\n",
    "            return page\n",
    "        except DisambiguationError as e:\n",
    "            # pick the first non-ambiguous option as fallback (best-effort)\n",
    "            # e.options is a list of possible titles\n",
    "            for option in e.options:\n",
    "                try:\n",
    "                    page = wikipedia.page(option, auto_suggest=False, preload=False)\n",
    "                    return page\n",
    "                except Exception:\n",
    "                    continue\n",
    "            return None\n",
    "        except PageError:\n",
    "            return None\n",
    "        except Exception:\n",
    "            # any other unexpected error -> None\n",
    "            return None\n",
    "\n",
    "    def scrape(\n",
    "        self,\n",
    "        query: str,\n",
    "        depth: str = \"moderate\",\n",
    "        full_text: bool = False,\n",
    "        max_pages_override: Optional[int] = None,\n",
    "        max_links_per_page_override: Optional[int] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Scrape wikipedia starting from `query`.\n",
    "        depth: one of 'shallow', 'moderate', 'deep'\n",
    "        full_text: whether to store full page.content (can be large). If False we store page.summary.\n",
    "        max_pages_override: optionally override configured max pages.\n",
    "        max_links_per_page_override: optionally override per-page link limit.\n",
    "\n",
    "        Returns:\n",
    "        {\n",
    "            \"start_query\": query,\n",
    "            \"depth\": depth,\n",
    "            \"pages\": { title: { \"title\", \"url\", \"summary\", \"content\"(opt), \"links\": [...] } },\n",
    "            \"edges\": [ (from_title, to_title), ... ],\n",
    "            \"visited_order\": [title1, title2, ...],\n",
    "            \"warnings\": [...],\n",
    "        }\n",
    "        \"\"\"\n",
    "        if depth not in self.DEPTH_CONFIG:\n",
    "            raise ValueError(f\"depth must be one of {list(self.DEPTH_CONFIG.keys())}\")\n",
    "\n",
    "        max_hops, cfg_max_pages, cfg_max_links = self.DEPTH_CONFIG[depth]\n",
    "        if max_pages_override is not None:\n",
    "            cfg_max_pages = int(max_pages_override)\n",
    "        if max_links_per_page_override is not None:\n",
    "            cfg_max_links = int(max_links_per_page_override)\n",
    "\n",
    "        # BFS-style crawl up to given hops, but stop when total pages >= cfg_max_pages\n",
    "        from collections import deque\n",
    "        queue: deque[Tuple[str, int]] = deque()  # (title, current_hop)\n",
    "        visited: Set[str] = set()\n",
    "        pages: Dict[str, Dict[str, Any]] = {}\n",
    "        edges: List[Tuple[str, str]] = []\n",
    "        warnings: List[str] = []\n",
    "\n",
    "        # Start by resolving the query to a page (try search then page)\n",
    "        try:\n",
    "            # Try direct page first (auto_suggest off to avoid surprises)\n",
    "            start_page = self._safe_get_page(query)\n",
    "            if start_page:\n",
    "                start_title = start_page.title\n",
    "            else:\n",
    "                # fallback: use wikipedia.search to find best match\n",
    "                search_results = wikipedia.search(query, results=5)\n",
    "                if not search_results:\n",
    "                    warnings.append(f\"No search results for query '{query}'.\")\n",
    "                    return {\n",
    "                        \"start_query\": query,\n",
    "                        \"depth\": depth,\n",
    "                        \"pages\": pages,\n",
    "                        \"edges\": edges,\n",
    "                        \"visited_order\": [],\n",
    "                        \"warnings\": warnings,\n",
    "                    }\n",
    "                start_title = search_results[0]\n",
    "        except Exception as e:\n",
    "            warnings.append(f\"Failed to resolve start query '{query}': {e}\")\n",
    "            return {\n",
    "                \"start_query\": query,\n",
    "                \"depth\": depth,\n",
    "                \"pages\": pages,\n",
    "                \"edges\": edges,\n",
    "                \"visited_order\": [],\n",
    "                \"warnings\": warnings,\n",
    "            }\n",
    "\n",
    "        queue.append((start_title, 0))\n",
    "\n",
    "        while queue and len(pages) < cfg_max_pages:\n",
    "            title, hop = queue.popleft()\n",
    "            if title in visited:\n",
    "                continue\n",
    "            # polite pause\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "            page = self._safe_get_page(title)\n",
    "            if page is None:\n",
    "                warnings.append(f\"Could not fetch page '{title}'. Skipping.\")\n",
    "                visited.add(title)\n",
    "                continue\n",
    "\n",
    "            visited.add(page.title)\n",
    "            # gather content\n",
    "            try:\n",
    "                summary = page.summary\n",
    "            except Exception:\n",
    "                summary = \"\"\n",
    "\n",
    "            content = None\n",
    "            if full_text:\n",
    "                try:\n",
    "                    content = page.content\n",
    "                except Exception:\n",
    "                    content = None\n",
    "\n",
    "            # get links (titles)\n",
    "            try:\n",
    "                raw_links = page.links or []\n",
    "            except Exception:\n",
    "                raw_links = []\n",
    "\n",
    "            # normalize / limit links\n",
    "            links = []\n",
    "            for l in raw_links:\n",
    "                if len(links) >= cfg_max_links:\n",
    "                    break\n",
    "                # simple exclusions: skip files and external links (page.links typically only titles)\n",
    "                if l.lower().startswith(\"file:\"):\n",
    "                    continue\n",
    "                links.append(l)\n",
    "\n",
    "            pages[page.title] = {\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url if hasattr(page, \"url\") else f\"https://en.wikipedia.org/wiki/{page.title.replace(' ', '_')}\",\n",
    "                \"summary\": summary,\n",
    "                \"content\": content,\n",
    "                \"links\": links,\n",
    "                \"hop\": hop,\n",
    "            }\n",
    "\n",
    "            # record edges\n",
    "            for to_title in links:\n",
    "                edges.append((page.title, to_title))\n",
    "\n",
    "            # enqueue next hop links if we still have hops left and page limit not reached\n",
    "            if hop < max_hops:\n",
    "                for to_title in links:\n",
    "                    if to_title not in visited and len(pages) + len(queue) < cfg_max_pages:\n",
    "                        queue.append((to_title, hop + 1))\n",
    "\n",
    "        visited_order = list(pages.keys())\n",
    "        return {\n",
    "            \"start_query\": query,\n",
    "            \"depth\": depth,\n",
    "            \"pages\": pages,\n",
    "            \"edges\": edges,\n",
    "            \"visited_order\": visited_order,\n",
    "            \"warnings\": warnings,\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = WikipediaScraper(delay_between_requests=0.2)\n",
    "    result = scraper.scrape(\"Graph neural network\", depth=\"deep\", full_text=True)\n",
    "    print(\"Visited:\", result[\"visited_order\"])\n",
    "    # inspect one page\n",
    "    for title, meta in result[\"pages\"].items():\n",
    "        print(\"TITLE:\", title)\n",
    "        print(\"URL:\", meta[\"url\"])\n",
    "        print(\"SUMMARY (first 300 chars):\", meta[\"summary\"][:300])\n",
    "        print(\"LINKS (first 10):\", meta[\"links\"][:10])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b9bd90",
   "metadata": {},
   "source": [
    "### Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be0d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146971/3591369090.py:53: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for r in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visited count: 24\n",
      "2307.00865v1 - A Survey on Graph Classification and Link Prediction based on GNN | ['Xingyu Liu', 'Juan Chen', 'Quan Wen']\n",
      "2007.06559v2 - Graph Structure of Neural Networks | ['Jiaxuan You', 'Jure Leskovec', 'Kaiming He']\n",
      "2011.01412v1 - Sampling and Recovery of Graph Signals based on Graph Neural Networks | ['Siheng Chen', 'Maosen Li', 'Ya Zhang']\n",
      "1908.00187v1 - Graph Neural Networks for Small Graph and Giant Network Representation Learning: An Overview | ['Jiawei Zhang']\n",
      "1902.10042v2 - Graph Neural Processes: Towards Bayesian Graph Neural Networks | ['Andrew Carr', 'David Wingate']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Dict, Any, List, Set, Tuple, Optional\n",
    "import arxiv   # pip install arxiv\n",
    "from collections import deque\n",
    "\n",
    "class ArxivScraper:\n",
    "    \"\"\"\n",
    "    Scraper that uses the `arxiv` python package to fetch paper metadata and\n",
    "    expands the search neighborhood by following author publications and\n",
    "    same-category papers as an approximation of 'linked' papers.\n",
    "\n",
    "    Depth presets:\n",
    "      - shallow:  (0 hops, top 1-3 papers)\n",
    "      - moderate: (1 hop, expand by authors + same category)\n",
    "      - deep:     (2 hops, larger expansion)\n",
    "    \"\"\"\n",
    "\n",
    "    # config tuples: (max_hops, max_papers_total, max_papers_per_seed)\n",
    "    DEPTH_CONFIG = {\n",
    "        \"shallow\":  (0,  3,  3),\n",
    "        \"moderate\": (1, 25,  8),\n",
    "        \"deep\":     (2, 80, 12),\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        results_per_query: int = 10,\n",
    "        delay_between_requests: float = 0.5,\n",
    "        max_results_returned_from_arxiv_call: int = 50,\n",
    "    ):\n",
    "        arxiv.timeout = 30  # seconds per request (module-level)\n",
    "        self.results_per_query = results_per_query\n",
    "        self.delay = delay_between_requests\n",
    "        # the arxiv library paginates; cap how many results we fetch for any helper call\n",
    "        self._max_results_per_call = max_results_returned_from_arxiv_call\n",
    "\n",
    "    def _sleep(self):\n",
    "        if self.delay and self.delay > 0:\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "    def _search(self, query: str, max_results: int = 10) -> List[arxiv.Result]:\n",
    "        \"\"\"\n",
    "        Run an arxiv search and return arxiv.Result objects (list).\n",
    "        \"\"\"\n",
    "        max_results = min(max_results, self._max_results_per_call)\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        results = []\n",
    "        try:\n",
    "            for r in search.results():\n",
    "                results.append(r)\n",
    "        except Exception:\n",
    "            # on transient network / parsing errors, return what we have\n",
    "            pass\n",
    "        return results\n",
    "\n",
    "    def _papers_to_meta(self, r: arxiv.Result) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert arxiv.Result to a serializable metadata dict.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"id\": getattr(r, \"entry_id\", None),\n",
    "            \"arxiv_id\": getattr(r, \"get_short_id\", lambda: None)() if hasattr(r, \"get_short_id\") else None,\n",
    "            \"title\": getattr(r, \"title\", None),\n",
    "            \"authors\": [a.name for a in getattr(r, \"authors\", [])] if getattr(r, \"authors\", None) else [],\n",
    "            \"summary\": getattr(r, \"summary\", None),\n",
    "            \"published\": getattr(r, \"published\", None),\n",
    "            \"updated\": getattr(r, \"updated\", None),\n",
    "            \"pdf_url\": next((l.href for l in getattr(r, \"links\", []) if getattr(l, \"title\", \"\") == \"pdf\"), None)\n",
    "                       or getattr(r, \"pdf_url\", None),\n",
    "            \"primary_category\": getattr(r, \"primary_category\", None) if hasattr(r, \"primary_category\") else None,\n",
    "            \"categories\": getattr(r, \"categories\", None) if hasattr(r, \"categories\") else None,\n",
    "            \"authors_inferred_query\": \", \".join([a.name for a in getattr(r, \"authors\", [])]) if getattr(r, \"authors\", None) else \"\",\n",
    "        }\n",
    "\n",
    "    def _get_by_author(self, author_name: str, max_results: int = 5) -> List[arxiv.Result]:\n",
    "        \"\"\"\n",
    "        Search for other papers by the same author. We craft an author: query.\n",
    "        \"\"\"\n",
    "        q = f'au:\"{author_name}\"'\n",
    "        return self._search(q, max_results=max_results)\n",
    "\n",
    "    def _get_by_category(self, category: str, max_results: int = 5) -> List[arxiv.Result]:\n",
    "        \"\"\"\n",
    "        Search for recent / relevant papers in the given category.\n",
    "        category should be like 'cs.AI' or 'stat.ML' — but we accept whatever arXiv expects.\n",
    "        We'll use the 'cat:' field if the user-specified category looks valid.\n",
    "        \"\"\"\n",
    "        # fallback: search for category token anywhere\n",
    "        q = f'cat:{category}'\n",
    "        results = self._search(q, max_results=max_results)\n",
    "        if not results:\n",
    "            # fallback to simple token search\n",
    "            results = self._search(category, max_results=max_results)\n",
    "        return results\n",
    "\n",
    "    def scrape(\n",
    "        self,\n",
    "        query: str,\n",
    "        depth: str = \"moderate\",\n",
    "        max_results_override: Optional[int] = None,\n",
    "        max_per_seed_override: Optional[int] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Scrape arXiv starting from a textual query and expand according to depth.\n",
    "\n",
    "        Returns a dict:\n",
    "          {\n",
    "            \"start_query\": query,\n",
    "            \"depth\": depth,\n",
    "            \"papers\": { unique_key: metadata_dict },\n",
    "            \"edges\": [ (from_id, to_id, relation) ],\n",
    "            \"visited_order\": [unique_key, ...],\n",
    "            \"warnings\": [...],\n",
    "          }\n",
    "\n",
    "        Notes:\n",
    "          - Because arXiv doesn't provide structured references/citations, 'edges' are derived\n",
    "            heuristically:\n",
    "              * \"author\": link from a paper to another paper by the same author discovered\n",
    "              * \"category\": link from a paper to another paper found in same primary category\n",
    "              * \"search_seed\": initial query -> paper\n",
    "          - Unique key used for nodes is arXiv short id if available else entry_id or title.\n",
    "        \"\"\"\n",
    "        if depth not in self.DEPTH_CONFIG:\n",
    "            raise ValueError(f\"depth must be one of {list(self.DEPTH_CONFIG.keys())}\")\n",
    "\n",
    "        max_hops, cfg_max_papers, cfg_max_per_seed = self.DEPTH_CONFIG[depth]\n",
    "        if max_results_override is not None:\n",
    "            cfg_max_papers = int(max_results_override)\n",
    "        if max_per_seed_override is not None:\n",
    "            cfg_max_per_seed = int(max_per_seed_override)\n",
    "\n",
    "        # Start: search for the query\n",
    "        warnings: List[str] = []\n",
    "        seed_results = self._search(query, max_results=self.results_per_query)\n",
    "        if not seed_results:\n",
    "            warnings.append(f\"No arXiv results for query '{query}'.\")\n",
    "            return {\n",
    "                \"start_query\": query,\n",
    "                \"depth\": depth,\n",
    "                \"papers\": {},\n",
    "                \"edges\": [],\n",
    "                \"visited_order\": [],\n",
    "                \"warnings\": warnings,\n",
    "            }\n",
    "\n",
    "        # BFS-like expansion using seeds, but nodes are unique by arxiv short id / entry_id / title\n",
    "        def node_key(r: arxiv.Result):\n",
    "            # prefer arXiv short id like '2101.00001'\n",
    "            try:\n",
    "                sid = r.get_short_id()\n",
    "            except Exception:\n",
    "                sid = None\n",
    "            if sid:\n",
    "                return sid\n",
    "            if getattr(r, \"entry_id\", None):\n",
    "                return getattr(r, \"entry_id\")\n",
    "            return getattr(r, \"title\", None)\n",
    "\n",
    "        queue = deque()\n",
    "        papers: Dict[str, Dict[str, Any]] = {}\n",
    "        edges: List[Tuple[str, str, str]] = []  # (from_key, to_key, relation)\n",
    "        visited: Set[str] = set()\n",
    "        visited_order: List[str] = []\n",
    "\n",
    "        # Enqueue seed results as hop=0\n",
    "        for r in seed_results[:cfg_max_per_seed]:\n",
    "            key = node_key(r)\n",
    "            queue.append((r, 0, \"seed\"))\n",
    "\n",
    "        while queue and len(papers) < cfg_max_papers:\n",
    "            r, hop, relation_from = queue.popleft()\n",
    "            key = node_key(r)\n",
    "            if not key or key in visited:\n",
    "                continue\n",
    "            self._sleep()\n",
    "            # Convert to metadata dict and save\n",
    "            meta = self._papers_to_meta(r)\n",
    "            meta[\"hop\"] = hop\n",
    "            papers[key] = meta\n",
    "            visited.add(key)\n",
    "            visited_order.append(key)\n",
    "\n",
    "            # record an edge from the seed query if relation_from == \"seed\"\n",
    "            if relation_from == \"seed\":\n",
    "                edges.append((f\"query::{query}\", key, \"search_seed\"))\n",
    "\n",
    "            # Heuristic expansions:\n",
    "            if hop < max_hops:\n",
    "                # 1) Add other papers by each author\n",
    "                authors = meta.get(\"authors\", []) or []\n",
    "                for author in authors:\n",
    "                    # limit how many author-search results we consider per author\n",
    "                    try:\n",
    "                        author_results = self._get_by_author(author, max_results=cfg_max_per_seed)\n",
    "                    except Exception:\n",
    "                        author_results = []\n",
    "                    for ar in author_results:\n",
    "                        k2 = node_key(ar)\n",
    "                        if not k2 or k2 in visited:\n",
    "                            continue\n",
    "                        edges.append((key, k2, \"author\"))\n",
    "                        # enqueue only if we still have budget\n",
    "                        if len(papers) + len(queue) < cfg_max_papers:\n",
    "                            queue.append((ar, hop + 1, \"author\"))\n",
    "\n",
    "                # 2) Add papers from same primary_category (if available)\n",
    "                prim_cat = meta.get(\"primary_category\") or (meta.get(\"categories\") and meta.get(\"categories\")[0])\n",
    "                if prim_cat:\n",
    "                    try:\n",
    "                        cat_results = self._get_by_category(prim_cat, max_results=cfg_max_per_seed)\n",
    "                    except Exception:\n",
    "                        cat_results = []\n",
    "                    for ar in cat_results:\n",
    "                        k2 = node_key(ar)\n",
    "                        if not k2 or k2 in visited:\n",
    "                            continue\n",
    "                        edges.append((key, k2, \"category\"))\n",
    "                        if len(papers) + len(queue) < cfg_max_papers:\n",
    "                            queue.append((ar, hop + 1, \"category\"))\n",
    "\n",
    "        # assemble the final structure\n",
    "        return {\n",
    "            \"start_query\": query,\n",
    "            \"depth\": depth,\n",
    "            \"papers\": papers,\n",
    "            \"edges\": edges,\n",
    "            \"visited_order\": visited_order,\n",
    "            \"warnings\": warnings,\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = ArxivScraper(results_per_query=5, delay_between_requests=0.2)\n",
    "    res = scraper.scrape(\"graph neural networks\", depth=\"moderate\")\n",
    "    print(\"Visited count:\", len(res[\"visited_order\"]))\n",
    "    for k in res[\"visited_order\"][:5]:\n",
    "        p = res[\"papers\"][k]\n",
    "        print(k, \"-\", p[\"title\"], \"|\", p[\"authors\"][:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a488a5",
   "metadata": {},
   "source": [
    "### Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb54825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SemanticScholarScraper\n",
    "\n",
    "Requires: pip install requests\n",
    "\n",
    "This class talks to the Semantic Scholar Academic Graph API (graph/v1).\n",
    "It supports:\n",
    " - start from a textual query (search) OR a paper id/DOI\n",
    " - expand via citations, references, authors' other papers, and recommendations\n",
    " - depth presets: shallow / moderate / deep (BFS on citation/reference graph)\n",
    " - simple rate-limit/backoff handling and optional API key support\n",
    "\n",
    "Notes:\n",
    " - Replace 'YOUR_API_KEY' with a real key if you have one (recommended).\n",
    " - Respect Semantic Scholar's terms of use & rate limits. See docs.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from typing import Dict, Any, List, Set, Tuple, Optional\n",
    "from collections import deque\n",
    "\n",
    "class SemanticScholarScraper:\n",
    "    # (max_hops, max_nodes_total, max_links_per_node)\n",
    "    DEPTH_CONFIG = {\n",
    "        \"shallow\":  (0,  2,  20),  # only seed results (2 papers)\n",
    "        \"moderate\": (1, 25,  30),  # follow citations/references 1 hop\n",
    "        \"deep\":     (2, 80,  40),  # up to 2 hops (be careful with quota)\n",
    "    }\n",
    "\n",
    "    BASE = \"https://api.semanticscholar.org/graph/v1\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        sleep: float = 0.0,\n",
    "        max_retries: int = 5,\n",
    "        backoff_factor: float = 1.5,\n",
    "        user_agent: str = \"LangGraph-SemanticScholarScraper/1.0\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        api_key: optional x-api-key for higher rate limits (recommended).\n",
    "        sleep: base sleep between requests (if you want additional throttling).\n",
    "        max_retries/backoff_factor: for 429/5xx handling.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.sleep = float(sleep)\n",
    "        self.max_retries = int(max_retries)\n",
    "        self.backoff_factor = float(backoff_factor)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\"User-Agent\": user_agent})\n",
    "        if api_key:\n",
    "            self.session.headers.update({\"x-api-key\": api_key})\n",
    "\n",
    "    # ---- low-level request with backoff ----\n",
    "    def _request(self, method: str, url: str, params: Dict[str, Any] = None, json: Dict[str, Any] = None):\n",
    "        delay = self.sleep\n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            if delay:\n",
    "                time.sleep(delay)\n",
    "            try:\n",
    "                resp = self.session.request(method, url, params=params, json=json, timeout=30)\n",
    "            except requests.RequestException as e:\n",
    "                # network-level error -> exponential backoff and retry\n",
    "                delay = max(delay * self.backoff_factor, 0.5) if delay else 0.5\n",
    "                continue\n",
    "            # handle 200\n",
    "            if resp.status_code == 200:\n",
    "                # return json or empty dict\n",
    "                try:\n",
    "                    return resp.json()\n",
    "                except ValueError:\n",
    "                    return {}\n",
    "            # handle rate limit\n",
    "            if resp.status_code in (429, 503):\n",
    "                # backoff then retry\n",
    "                # try to parse Retry-After header\n",
    "                ra = resp.headers.get(\"Retry-After\")\n",
    "                if ra:\n",
    "                    try:\n",
    "                        sleep_for = float(ra)\n",
    "                    except Exception:\n",
    "                        sleep_for = delay if delay else 1.0\n",
    "                else:\n",
    "                    sleep_for = delay if delay else 1.0\n",
    "                # increase delay for next round\n",
    "                delay = (delay or 1.0) * self.backoff_factor\n",
    "                time.sleep(sleep_for)\n",
    "                continue\n",
    "            # client error -> return None\n",
    "            return None\n",
    "        # exhausted retries\n",
    "        return None\n",
    "\n",
    "    # ---- convenience helpers for API endpoints ----\n",
    "    def _search_papers(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        uses: GET /paper/search?query=...&limit=...\n",
    "        fields: we request a compact set of fields helpful for graph navigation\n",
    "        \"\"\"\n",
    "        url = f\"{self.BASE}/paper/search\"\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"limit\": limit,\n",
    "            \"fields\": \"paperId,title,abstract,authors,year,venue,doi,externalIds,fieldsOfStudy,url,citationCount,referenceCount\"\n",
    "        }\n",
    "        data = self._request(\"GET\", url, params=params)\n",
    "        if not data:\n",
    "            return []\n",
    "        # Semantic Scholar returns 'data' key with list of hits\n",
    "        return data.get(\"data\", []) if isinstance(data, dict) else []\n",
    "\n",
    "    def _get_paper(self, paper_id: str, fields: str = None) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        GET /paper/{paperId}?fields=...\n",
    "        Accepts arXiv id, DOI, S2 id etc.\n",
    "        \"\"\"\n",
    "        if fields is None:\n",
    "            fields = \",\".join([\n",
    "                \"paperId\",\"title\",\"abstract\",\"authors\",\"year\",\"venue\",\"doi\",\"externalIds\",\n",
    "                \"fieldsOfStudy\",\"url\",\"citationCount\",\"referenceCount\",\"influentialCitationCount\"\n",
    "            ])\n",
    "        url = f\"{self.BASE}/paper/{requests.utils.quote(paper_id, safe='')}\"\n",
    "        params = {\"fields\": fields}\n",
    "        return self._request(\"GET\", url, params=params)\n",
    "\n",
    "    def _get_related(self, paper_id: str, relation: str = \"citations\", limit: int = 25) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        relation: 'citations' or 'references' or 'recommendations' (recommendations endpoint differs)\n",
    "        For citations/references: GET /paper/{paperId}/citations?fields=paperId,title,authors,...\n",
    "        \"\"\"\n",
    "        if relation not in (\"citations\", \"references\"):\n",
    "            raise ValueError(\"relation must be 'citations' or 'references'\")\n",
    "        url = f\"{self.BASE}/paper/{requests.utils.quote(paper_id, safe='')}/{relation}\"\n",
    "        params = {\n",
    "            \"limit\": limit,\n",
    "            \"fields\": \"citingPaper.paperId,citingPaper.title,citingPaper.authors,citingPaper.year,citingPaper.venue,citingPaper.doi,citingPaper.citationCount\"\n",
    "        } if relation == \"citations\" else {\n",
    "            \"limit\": limit,\n",
    "            \"fields\": \"referencedPaper.paperId,referencedPaper.title,referencedPaper.authors,referencedPaper.year,referencedPaper.venue,referencedPaper.doi,referencedPaper.citationCount\"\n",
    "        }\n",
    "        data = self._request(\"GET\", url, params=params)\n",
    "        if not data:\n",
    "            return []\n",
    "        # Structure differs slightly: items may be wrapped (citingPaper/referencedPaper)\n",
    "        items = data.get(\"data\", []) if isinstance(data, dict) else []\n",
    "        flattened = []\n",
    "        for it in items:\n",
    "            # try both shapes\n",
    "            if \"citingPaper\" in it:\n",
    "                flattened.append(it[\"citingPaper\"])\n",
    "            elif \"referencedPaper\" in it:\n",
    "                flattened.append(it[\"referencedPaper\"])\n",
    "            else:\n",
    "                flattened.append(it)\n",
    "        return flattened\n",
    "\n",
    "    def _get_author_papers(self, author_id: str, limit: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        GET /author/{authorId}/papers - returns papers by an author.\n",
    "        We'll request compact fields.\n",
    "        \"\"\"\n",
    "        url = f\"{self.BASE}/author/{requests.utils.quote(author_id, safe='')}/papers\"\n",
    "        params = {\n",
    "            \"limit\": limit,\n",
    "            \"fields\": \"paperId,title,year,venue,doi,citationCount\"\n",
    "        }\n",
    "        data = self._request(\"GET\", url, params=params)\n",
    "        if not data:\n",
    "            return []\n",
    "        return data.get(\"data\", []) if isinstance(data, dict) else []\n",
    "\n",
    "    # ---- top-level scrape API ----\n",
    "    def scrape(\n",
    "        self,\n",
    "        query_or_id: str,\n",
    "        depth: str = \"moderate\",\n",
    "        use_citations: bool = True,\n",
    "        use_references: bool = True,\n",
    "        use_author_papers: bool = True,\n",
    "        use_recommendations: bool = False,\n",
    "        max_per_seed_override: Optional[int] = None,\n",
    "        max_nodes_override: Optional[int] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        query_or_id: free-text query (search) OR a paper id/DOI (detect heuristically).\n",
    "        depth: shallow/moderate/deep\n",
    "        Returns:\n",
    "          {\n",
    "            \"start\": query_or_id,\n",
    "            \"depth\": depth,\n",
    "            \"papers\": { paperId: metadata_dict },\n",
    "            \"edges\": [ (from_paperId, to_paperId, relation) ],\n",
    "            \"visited_order\": [...],\n",
    "            \"warnings\": [...]\n",
    "          }\n",
    "        \"\"\"\n",
    "        if depth not in self.DEPTH_CONFIG:\n",
    "            raise ValueError(f\"depth must be one of {list(self.DEPTH_CONFIG.keys())}\")\n",
    "\n",
    "        max_hops, cfg_max_nodes, cfg_max_per_seed = self.DEPTH_CONFIG[depth]\n",
    "        if max_per_seed_override is not None:\n",
    "            cfg_max_per_seed = int(max_per_seed_override)\n",
    "        if max_nodes_override is not None:\n",
    "            cfg_max_nodes = int(max_nodes_override)\n",
    "\n",
    "        warnings: List[str] = []\n",
    "        papers: Dict[str, Dict[str, Any]] = {}\n",
    "        edges: List[Tuple[str, str, str]] = []\n",
    "        visited: Set[str] = set()\n",
    "        visited_order: List[str] = []\n",
    "\n",
    "        # Decide if input is a paper id (contains '/' or ':' for DOI/arXiv or looks like S2 id)\n",
    "        is_paper_id = any(tok in query_or_id for tok in (\"/\", \":\", \"arXiv\")) or len(query_or_id) > 10 and \" \" not in query_or_id\n",
    "\n",
    "        seeds: List[Dict[str, Any]] = []\n",
    "        if is_paper_id:\n",
    "            res = self._get_paper(query_or_id)\n",
    "            if res:\n",
    "                seeds.append(res)\n",
    "            else:\n",
    "                # fallback: attempt search\n",
    "                seeds = self._search_papers(query_or_id, limit=cfg_max_per_seed)\n",
    "        else:\n",
    "            seeds = self._search_papers(query_or_id, limit=cfg_max_per_seed)\n",
    "\n",
    "        if not seeds:\n",
    "            warnings.append(f\"No seeds found for '{query_or_id}'\")\n",
    "            return {\n",
    "                \"start\": query_or_id,\n",
    "                \"depth\": depth,\n",
    "                \"papers\": papers,\n",
    "                \"edges\": edges,\n",
    "                \"visited_order\": visited_order,\n",
    "                \"warnings\": warnings\n",
    "            }\n",
    "\n",
    "        # BFS queue of tuples (paper_meta_dict, hop, relation_from)\n",
    "        queue = deque()\n",
    "        for s in seeds[:cfg_max_per_seed]:\n",
    "            queue.append((s, 0, \"seed\"))\n",
    "\n",
    "        def get_key(pp: Dict[str, Any]) -> Optional[str]:\n",
    "            return pp.get(\"paperId\") or pp.get(\"doi\") or pp.get(\"url\") or pp.get(\"title\")\n",
    "\n",
    "        while queue and len(papers) < cfg_max_nodes:\n",
    "            node_meta, hop, relation_from = queue.popleft()\n",
    "            key = get_key(node_meta)\n",
    "            if not key:\n",
    "                continue\n",
    "            if key in visited:\n",
    "                continue\n",
    "            # normalize: if only seed was from search, fetch full paper to get counts & ids\n",
    "            if \"paperId\" not in node_meta or not node_meta.get(\"paperId\"):\n",
    "                # try fetch by title/doi via search to get canonical id\n",
    "                # try short search by title (1 result)\n",
    "                title = node_meta.get(\"title\")\n",
    "                if title:\n",
    "                    search_hits = self._search_papers(title, limit=1)\n",
    "                    if search_hits:\n",
    "                        node_meta = search_hits[0]\n",
    "            # nunmp\n",
    "            pid = node_meta.get(\"paperId\") or node_meta.get(\"doi\") or node_meta.get(\"url\") or node_meta.get(\"title\")\n",
    "            if not pid:\n",
    "                continue\n",
    "\n",
    "            # Fetch full paper metadata if not detailed already\n",
    "            full = self._get_paper(pid)\n",
    "            if full is None:\n",
    "                # keep minimal meta if fetching failed\n",
    "                full = node_meta\n",
    "\n",
    "            # store node\n",
    "            papers_key = full.get(\"paperId\") or full.get(\"doi\") or full.get(\"url\") or full.get(\"title\")\n",
    "            if not papers_key:\n",
    "                continue\n",
    "            papers[papers_key] = {\n",
    "                \"paperId\": full.get(\"paperId\"),\n",
    "                \"title\": full.get(\"title\"),\n",
    "                \"abstract\": full.get(\"abstract\"),\n",
    "                \"authors\": [{\"name\": a.get(\"name\"), \"authorId\": a.get(\"authorId\")} for a in full.get(\"authors\", [])] if isinstance(full.get(\"authors\"), list) else [],\n",
    "                \"year\": full.get(\"year\"),\n",
    "                \"venue\": full.get(\"venue\"),\n",
    "                \"doi\": full.get(\"doi\"),\n",
    "                \"url\": full.get(\"url\"),\n",
    "                \"citationCount\": full.get(\"citationCount\"),\n",
    "                \"referenceCount\": full.get(\"referenceCount\"),\n",
    "                \"fieldsOfStudy\": full.get(\"fieldsOfStudy\"),\n",
    "                \"hop\": hop,\n",
    "            }\n",
    "            visited.add(papers_key)\n",
    "            visited_order.append(papers_key)\n",
    "\n",
    "            # record edge from seed query if needed\n",
    "            if relation_from == \"seed\":\n",
    "                edges.append((f\"query::{query_or_id}\", papers_key, \"search_seed\"))\n",
    "\n",
    "            # If we can expand, push citations/references/author papers\n",
    "            if hop < max_hops:\n",
    "                # 1) citations\n",
    "                if use_citations:\n",
    "                    try:\n",
    "                        related = self._get_related(papers_key, relation=\"citations\", limit=cfg_max_per_seed)\n",
    "                    except Exception:\n",
    "                        related = []\n",
    "                    for r in related[:cfg_max_per_seed]:\n",
    "                        k2 = get_key(r)\n",
    "                        if not k2 or k2 in visited:\n",
    "                            continue\n",
    "                        edges.append((papers_key, r.get(\"paperId\") or k2, \"cites_me\"))  # r cites this paper\n",
    "                        if len(papers) + len(queue) < cfg_max_nodes:\n",
    "                            queue.append((r, hop+1, \"citation\"))\n",
    "\n",
    "                # 2) references\n",
    "                if use_references:\n",
    "                    try:\n",
    "                        related = self._get_related(papers_key, relation=\"references\", limit=cfg_max_per_seed)\n",
    "                    except Exception:\n",
    "                        related = []\n",
    "                    for r in related[:cfg_max_per_seed]:\n",
    "                        k2 = get_key(r)\n",
    "                        if not k2 or k2 in visited:\n",
    "                            continue\n",
    "                        edges.append((papers_key, r.get(\"paperId\") or k2, \"references\"))  # this paper -> referenced paper\n",
    "                        if len(papers) + len(queue) < cfg_max_nodes:\n",
    "                            queue.append((r, hop+1, \"reference\"))\n",
    "\n",
    "                # 3) author papers (optional)\n",
    "                if use_author_papers:\n",
    "                    for a in papers[papers_key].get(\"authors\", [])[:3]:  # limit to top 3 authors to prevent explosion\n",
    "                        aid = a.get(\"authorId\")\n",
    "                        if not aid:\n",
    "                            continue\n",
    "                        try:\n",
    "                            author_papers = self._get_author_papers(aid, limit=min(10, cfg_max_per_seed))\n",
    "                        except Exception:\n",
    "                            author_papers = []\n",
    "                        for ap in author_papers[:cfg_max_per_seed]:\n",
    "                            k2 = get_key(ap)\n",
    "                            if not k2 or k2 in visited:\n",
    "                                continue\n",
    "                            edges.append((papers_key, ap.get(\"paperId\") or k2, \"author\"))\n",
    "                            if len(papers) + len(queue) < cfg_max_nodes:\n",
    "                                queue.append((ap, hop+1, \"author\"))\n",
    "\n",
    "                # 4) recommendations (optional, omitted by default)\n",
    "                if use_recommendations:\n",
    "                    # The recommendations endpoint is separate; if you want it, you can add calls here.\n",
    "                    pass\n",
    "\n",
    "        return {\n",
    "            \"start\": query_or_id,\n",
    "            \"depth\": depth,\n",
    "            \"papers\": papers,\n",
    "            \"edges\": edges,\n",
    "            \"visited_order\": visited_order,\n",
    "            \"warnings\": warnings\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2255db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web-research (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
