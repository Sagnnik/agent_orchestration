{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a646932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b172007",
   "metadata": {},
   "source": [
    "### Manual tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1aaf0041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call:  [{'name': 'add', 'args': {'x': 35, 'y': 156}, 'id': '21cf2abf-441a-44e2-a0ee-eea771c2802d', 'type': 'tool_call'}]\n",
      "content='The result of adding 35 and 156 is 191.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-11-05T18:09:30.217001955Z', 'done': True, 'done_reason': 'stop', 'total_duration': 575612875, 'load_duration': 381125751, 'prompt_eval_count': 93, 'prompt_eval_duration': 26561543, 'eval_count': 14, 'eval_duration': 155347786, 'model_name': 'llama3.2', 'model_provider': 'ollama'} id='lc_run--c40624d0-b07b-4636-81a8-04eea57cc953-0' usage_metadata={'input_tokens': 93, 'output_tokens': 14, 'total_tokens': 107}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.messages import HumanMessage, ToolMessage, AIMessage\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "#get the model\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "model.invoke(\"tell me a joke\")\n",
    "\n",
    "#define the tool\n",
    "@tool\n",
    "def add(x:int, y:int) -> int:\n",
    "    \"\"\"Function to add 2 numbers\"\"\"\n",
    "    return x+y\n",
    "\n",
    "messages = [HumanMessage(\"add 35 + 156\")]\n",
    "#call the model by binding the tool\n",
    "tool_llm = model.bind_tools(tools=[add])\n",
    "response = tool_llm.invoke(messages)\n",
    "print(\"Tool call: \",response.tool_calls)\n",
    "# manually call the tool and append the ToolMessage\n",
    "for tc in response.tool_calls:\n",
    "    name = tc[\"name\"]\n",
    "    args = tc['args']\n",
    "    result = add.invoke(args)\n",
    "    messages += [response, ToolMessage(content=str(result), tool_call_id=tc['id'])]\n",
    "\n",
    "final = tool_llm.invoke(messages)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dea050d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Mumbai is sunny, with a temperature of 72 degrees.\n"
     ]
    }
   ],
   "source": [
    "#fake ai message mimicking a tool call\n",
    "ai_msg = AIMessage(\n",
    "    content=[],\n",
    "    tool_calls=[{\n",
    "        \"name\": \"get_weather\",\n",
    "        \"args\": {\"location\": \"Mumbai\"},\n",
    "        \"id\": \"call_123\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Finds the weather given a location\"\"\"\n",
    "    return \"Sunny, 72 degress\"\n",
    "\n",
    "tool_msg = ToolMessage(content=\"Sunny, 72 degress\", tool_call_id=\"call_123\")\n",
    "\n",
    "messages = [HumanMessage(\"What is the weather in Mumbai?\"), ai_msg, tool_msg]\n",
    "tool_llm = model.bind_tools(tools=[get_weather])\n",
    "response = tool_llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0503f7",
   "metadata": {},
   "source": [
    "### How pipeline works in LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aec061f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "prompt = (SystemMessagePromptTemplate.from_template(\"You are a helpful assistant\") + \"Give me a small report about {topic}\")\n",
    "output_parser = StrOutputParser()\n",
    "model_chain = prompt | model | output_parser\n",
    "\n",
    "response = model_chain.invoke({\"topic\": \"LLM on edge device\"})\n",
    "# This directly gives out the response content which is in str format\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701e77d",
   "metadata": {},
   "source": [
    "### .bind -> returns a new runnable with altered parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fde702bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob the Builder is a fictional character in a children's television series created by Keith Chapman. He is a can-do, optimistic construction worker who solves problems and builds things with his team, featuring talking machines, in the fictional town of Sunflower Valley.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Answer the question concisely: {question}\")\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "model_chain = prompt | model.bind(options={\"temperature\":0.7}) | output_parser\n",
    "\n",
    "result = model_chain.invoke({\"question\": \"Who was bob the builder\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1154b",
   "metadata": {},
   "source": [
    "### Getting structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "913c3491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Inception' release=2010 summary=\"A mind-bending sci-fi action film that delves into the concept of shared dreaming, where a skilled thief is tasked with planting an idea in someone's mind instead of stealing one. The film follows Cobb (Leonardo DiCaprio), a complex and troubled individual who must navigate multiple levels of dreams within dreams to achieve his mission. Alongside his team, including Arthur (Joseph Gordon-Levitt), Ariadne (Ellen Page), Eames (Tom Hardy), and Saito (Ken Watanabe), Cobb embarks on a perilous journey that blurs the lines between reality and fantasy. With its innovative visual effects, intricate plot twists, and thought-provoking themes, Inception is a cinematic masterpiece that will keep you on the edge of your seat.\" rating=9.5\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "class Movie(BaseModel):\n",
    "    title: str = Field(..., description=\"title of the movie\")\n",
    "    release: int = Field(..., description=\"The year of release\")\n",
    "    summary: str = Field(..., description=\"Short summary of the Movie\")\n",
    "    rating: float = Field(..., description=\"The rating of the movie\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a movie review agent and you provide a detailed description of the movie.\"),\n",
    "    (\"user\", \"How was the {name} movie?\")\n",
    "])\n",
    "\n",
    "model_structured = model.with_structured_output(Movie)\n",
    "model_chain = prompt | model_structured \n",
    "\n",
    "response = model_chain.invoke({\"name\": \"Inception\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94081032",
   "metadata": {},
   "source": [
    "#### Concept of Runnable in LC\n",
    "A Runnable is anything that can *run* to produce an output. I can use these functions with *runnables*: invoke(), batch(), stream(), etc.\n",
    "Types of runnables\n",
    "- RunnableLambda -> Wraps a function as runnable\n",
    "- RunnableMap -> Runs multiple runnables in pararell\n",
    "- RunnableSequence -> same as the `|` operator  \n",
    "etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22bf6b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created runnable: 12\n",
      "With Mapping Lambdas:  {'square': 25, 'cube': 125}\n"
     ]
    }
   ],
   "source": [
    "# Creating runnables\n",
    "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
    "add_one = RunnableLambda(lambda x: x+1)\n",
    "double = RunnableLambda(lambda x: x*2)\n",
    "\n",
    "pipeline = add_one | double\n",
    "print(\"created runnable:\", pipeline.invoke(5))\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"square\": RunnableLambda(lambda x: x*x),\n",
    "    \"cube\": RunnableLambda(lambda x: x*x*x)\n",
    "})\n",
    "\n",
    "print(\"With Mapping Lambdas: \", chain.invoke(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dacab0",
   "metadata": {},
   "source": [
    "### Streamimg Responses\n",
    "- with agent.stream()\n",
    "- with model.stream() -> after passing the model through the tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0123c2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming on updates...\n",
      "step: model\n",
      "content: [{'type': 'tool_call', 'id': 'b3e3bbe4-af56-46bf-87fa-3c20fd65a12f', 'name': 'get_weather', 'args': {'city': 'SF'}}]\n",
      "step: tools\n",
      "content: [{'type': 'text', 'text': \"It's always sunny in SF!\"}]\n",
      "step: model\n",
      "content: [{'type': 'text', 'text': 'The current weather conditions in San Francisco are mostly sunny with a high of 68째F (20째C) and a low of 55째F (13째C).'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.config import get_stream_writer\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\" Get the weather for a given city \"\"\"\n",
    "    writer = get_stream_writer()\n",
    "    writer(f\"Looking up data for the city: {city}\")\n",
    "    writer(f\"Accquired data for the city: {city}\")\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "prompt = {\"messages\":[HumanMessage(content=\"Can you give me the weather at SF\")]}\n",
    "agent = create_agent(model, tools=[get_weather])\n",
    "\n",
    "print(\"Streaming on updates...\")\n",
    "for chunk in agent.stream(prompt, stream_mode=\"updates\"):\n",
    "    for step, data in chunk.items():\n",
    "        print(f\"step: {step}\")\n",
    "        print(f\"content: {data['messages'][-1].content_blocks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c9d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "with-langgraph (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
